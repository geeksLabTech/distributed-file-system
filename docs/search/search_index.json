{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This project uses Kademlia as its starting point. Kademlia is a distributed hash table that enables millions of computers to automatically organize themselves into a network, communicate with other computers in the network, and share resources.</p> <p>Several approaches and ideas proposed by Kademlia are used as a foundation to expand the functionalities of the project, while others are used in the same way as Kademlia.</p> <p>Once a file system is in place, it is integrated with AutoGOAL to efficiently solve machine learning problems in terms of performance, network usage, and storage.</p>"},{"location":"autodiscovery/","title":"Autodiscovery","text":"<p>For the autodiscovery mechanism, a <code>Thread</code> is also created that sends a heartbeat to the broadcast addresses of each NIC (Network Interface Card) on the host, with a broadcast source identifier and the IP and port \"<code>dfs ip port</code>\". Whenever a new node joins the network, it listens to the broadcasts to discover neighbors. This mechanism is accessible from the client, making the connection transparent to the user.</p>"},{"location":"autodiscovery/#disadvantages-of-this-approach","title":"Disadvantages of this approach","text":"<p>Since broadcast is used for autodiscovery, it is only possible within a local network or by using one (or multiple) PCs as a bridge between different subnets. If, for some reason, there is no way to connect computers locally, they will not be able to discover each other. However, since the system is designed for connecting different workstations, it is assumed that they will be on the same local network, making this approach feasible.</p>"},{"location":"cap-analysis/","title":"CAP Theorem Analysis","text":"<p>The CAP theorem, also known as Brewer's theorem, is a fundamental concept in distributed systems that states that it is impossible for a distributed data store to simultaneously provide consistency (C), availability (A), and partition tolerance (P).</p> <ul> <li> <p>Consistency (C): Consistency refers to all nodes in a distributed system having the same view of the data at the same time. In this system, achieving strict consistency across all nodes is not a priority. Instead, eventual consistency is guaranteed, which means that over time, all nodes will converge to the same state. Nodes periodically exchange information and update their routing tables to achieve this convergence.</p> </li> <li> <p>Availability (A): Availability implies that the system remains responsive and accessible to users even in the presence of node failures or network partitions. The system prioritizes availability by ensuring that nodes can continue to operate and provide services even when some nodes are unavailable or unreachable. This is achieved through data redundancy and replication, where multiple copies of data are stored on different nodes.</p> </li> <li> <p>Partition Tolerance (P): Partition tolerance refers to the system's ability to continue functioning and providing services despite network partitions or failures. The system is designed to be partition-tolerant, allowing the network to continue operating and maintaining its functionality even when nodes are temporarily disconnected or isolated due to network issues.</p> </li> </ul> <p>All these points are analyzed assuming that the system's recovery capacity is not exceeded. If this point were exceeded, the system's availability would start to be affected. However, this would only happen in the case of a certain number of simultaneous failures since the system can recover from eventual failures without any problems.</p>"},{"location":"client/","title":"Client","text":"<p>A client has been developed with the following features:</p> <ul> <li>It can receive entry points to the network, known as bootstrap nodes, to establish a direct connection with the file system.</li> <li>It has a self-discovery mechanism that listens for broadcast messages on all NICs of the PC. If it doesn't receive any bootstrap node or can't establish a connection with any, it can use this functionality to automatically find a node.</li> <li>It has the ability to automatically discover other nodes by connecting to an existing node and obtaining information about its neighbors.</li> <li>It handles errors related to network instability or unexpected node failures. The user can set the number of connection retries when the connection to a node is lost. It also maintains a queue of known nodes. When the connection retries with a node are exhausted, it is removed from the queue and the next one is tried. It is also possible to specify whether to use the self-discovery mechanism when the queue is empty.</li> </ul>"},{"location":"description/","title":"System Description","text":"<p>The system utilizes a similar approach to Kademlia, where keys are stored as 160-bit values. Each node in the network has a unique ID, and the (key, value) pairs are stored on nodes with \"close\" IDs using the XOR metric proposed by Kademlia.</p> <p>The system treats nodes as leaves of a binary tree, where the position of each node is determined by the shortest unique prefix of its ID. These nodes store contact information to route query messages.</p> <p>The Kademlia protocol is used, which includes the PING, STORE, FINDNODE, and FIND-VALUE Remote Procedure Calls (RPCs). Additionally, other RPCs are implemented, such as CONTAINS, BOOTSTRAPPABLE-NEIGHBOR, GET, GET-FILE-CHUNKS, UPLOAD-FILE, SET-KEY, CHECK-IF-NEW-VALUE-EXIST, GET-FILE-CHUNK-VALUE, GET-FILE-CHUNK-LOCATION, FIND-CHUNK-LOCATION, and FIND-NEIGHBORS.</p> <ul> <li><code>GET</code>: Retrieves information identified by the key of a file, returns a list with the keys of each chunk.</li> <li><code>UPLOAD-FILE</code>: Uploads the file to the file system, divides it into chunks, and saves the file's metadata in a way that unifies all the chunks.</li> <li><code>GET-FIND-CHUNK-VALUE</code>: Returns the value associated with the chunk.</li> <li><code>GET-FIND-CHUNK-LOCATION</code>: Receives the key of a chunk and returns a tuple (IP, port) where it is located.</li> <li><code>FIND-NEIGHBORS</code>: Returns the neighbors to the node according to proximity based on the XOR metric. Additionally, the servers use the following RPCs:</li> <li><code>CONTAINS</code>: Determines if a key is in a node. This is used for both information replication and to find if a node has the desired information.</li> <li><code>GET-FILE-CHUNKS</code>: Retrieves the list of chunk locations for the information.</li> <li><code>SET-KEY</code>: Stores a key-value pair in the system.</li> <li><code>DELETE</code>: Deletes a file from the network.</li> <li><code>GET-ALL-FILE-NAMES</code>: Performs the function of the <code>ls</code> command in Linux, returns a list of all files in the system (all metadata keys).</li> </ul> <p>The system also includes a persistence module called <code>PersistentStorage</code>, which handles data read and write operations. It uses the following paths:</p> <ul> <li><code>static/metadata</code>: stores the filenames representing the hashes of the data divided into chunks of up to 1000kb. These files contain Python lists saved with pickle, which contain the hashes of each chunk obtained by splitting the data.</li> <li><code>static/keys</code>: stores the filenames representing the hashes of the stored data, either complete data or chunks. These files contain the corresponding hashes in bytes.</li> <li><code>static/values</code>: stores the filenames representing the hashes of the stored chunks, excluding the hashes of undivided data.</li> <li><code>timestamps</code>: The file names representing the hashes of the stored data are stored, similar to the keys path, but they contain a Python dictionary saved with Pickle. This dictionary has keys such as date with the value datetime.now() and republish with a boolean value. This information is used to keep track of the last time a key was accessed and to determine if it is necessary to republish the information in order to maintain the system's property that the closest nodes to it are the ones that have it.</li> </ul> <p>When a <code>(key, value)</code> pair is received as bytes, the <code>PersistentStorage</code> module encodes the key using <code>base64.urlsafe_b64encode</code> to obtain a string that can be used as a filename. Then, a file is written with that name in the <code>keys</code> and <code>values</code> paths, where the key is saved as bytes in the keys file and the value is saved as bytes in the values file. In the case of storing metadata, the value as bytes is written in the metadata path. In both cases, a corresponding file is also created in the timestamps path.</p> <p>Before writing a value or metadata, a dictionary is created with the following keys:</p> <ul> <li><code>integrity</code>: Used to determine if the file is corrupt. If it is, it is never returned.</li> <li><code>value</code>: Value to be written.</li> <li><code>integrity_date</code>: The moment when the integrity is first set. If a certain amount of time has passed since this date and integrity is still False, the file is automatically deleted.</li> <li><code>key_name</code>: String representing the file's key. It is used to retrieve the original file names if the <code>ls</code> command is executed on the client.</li> <li><code>last_write</code>: The moment when the file was last written. It is used to handle cases where, after a partition, if the same keys were written in both partitions, the most recently written values are maintained after network recovery.</li> </ul> <p>Routing in the system utilizes a routing table structure similar to Kademlia. The routing table is a binary tree composed of k-buckets. Each k-bucket contains nodes with a common prefix in their IDs, and the prefix determines the position of the k-bucket in the binary tree. The k-buckets cover different parts of the ID space and together cover the entire 160-bit ID space without overlap. Nodes are dynamically assigned to k-buckets as needed.</p> <p>To ensure data replication, nodes must periodically republish keys. This is because some of the k nodes that initially obtain a key-value pair may leave the network, and new nodes with IDs closer to the key may join the network. Therefore, nodes that store the key-value pair must republish it to ensure it is available on the k nodes closest to the key.</p> <p>When a client requests a certain value from the system, they are returned a list of locations of the different data chunks, which may be on different PCs. The client then establishes a connection with the PC closest to the information to retrieve the data and unify it. Once a node sends information, it marks the file as pending republishing and updates its timestamp, informing neighbors that they should also replicate the information.</p> <p>When a server discovers a new node, for each key in storage, the system retrieves the k closest nodes. If the new node is closer than the furthest node in that list, and the node for this server is closer than the closest node in that list, then the key/value pair is stored on the new node.</p> <p>The server spawns a thread for each connection, supporting multiple clients.</p> <p>When the client starts uploading a file, the data is divided into chunks of a maximum size of 1000kb, and they are stored in the file system with integrity set to False. Then, the metadata of the file is stored, which contains the necessary information to unify the chunks and reconstruct the original file, also with integrity set to False. Once it is confirmed that all the chunks and metadata could be successfully stored, the integrity of each chunk in the network is verified, followed by the integrity of the metadata. In case an error occurs at any point in the process, everything stored is deleted, mimicking a rollback effect.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#store-a-value-in-the-server","title":"Store a Value in the server","text":""},{"location":"examples/#note-make-sure-that-there-exist-at-least-a-server-in-the-local-network","title":"Note: Make sure that there exist at least a server in the local network","text":"<pre><code>from kade_drive.cli import ClientSession\nclient = ClientSession()\nclient.connect()\nresponse,_ = client.put(4, 5)\nvalue,_ = client.get(4)\nassert value == 5\n</code></pre>"},{"location":"examples/#storing-a-dataframe","title":"Storing a Dataframe","text":"<pre><code>from sklearn.datasets import load_diabetes\nimport pandas as pd\nimport pickle\nfrom client import ClientSession\nfrom time import sleep\nX, y = load_diabetes(return_X_y=True)\nX = pd.DataFrame(X)\ny = pd.DataFrame(y)\nX_pickle = pickle.dumps(X)\ninitial_bootstrap_nodes = []\nclient_session = ClientSession(initial_bootstrap_nodes)\nclient_session.connect(use_broadcast_if_needed=True)\nresponse, _ = client_session.put(\"dataset_diabetes\", X_pickle)\nsleep(5)\nvalue_getted, _ = client_session.get(\"dataset_diabetes\")\nassert X.equals(value_getted)\n</code></pre>"},{"location":"fault-tolerance/","title":"Fault Tolerance","text":"<p>Nodes maintain information about other nearby nodes in the network by storing these nodes in their routing table. Each node has a list of <code>k-buckets</code> that contain the contact details of other nodes in the network, categorized based on their proximity in the ID space.</p> <p>When a node becomes unresponsive or disconnects, other nodes in the network detect the lack of response after a specified period of time. At that point, the failed node is considered inactive.</p> <p>When a node detects the inactivity of another node, it updates its routing table by removing the entry of the failed node. Additionally, the node performs a series of actions to maintain connectivity and redundancy in the network.</p> <ul> <li> <p>Data Replication: If the failed node was storing data, other nodes in the network can assume the responsibility of maintaining replicas of that data. This way, the data remains available even if the original node becomes disconnected. The nodes that will take up this responsibility will be those that are closest using the same XOR metric with IDs to keep the network as optimized as possible.</p> </li> <li> <p>Routing Information Update: Nodes that had the failed node in their routing table update that entry by removing it. This way, the nodes avoid sending messages or performing actions towards a node that is no longer available. In the event that a node runs out of neighbors, it can execute a broadcast to try to discover other nodes in the network.</p> </li> <li> <p>Contact expiration ensures that the information stored in the network remains accessible even when individual nodes fail. By removing inactive nodes from the buckets, it ensures that routing paths are updated and kept efficient.</p> </li> </ul>"},{"location":"installation-setup/","title":"Installation","text":"<p>To install the <code>kade-drive</code> library, you can execute the following command in the command line:</p> <pre><code>pip install kade-drive\n</code></pre>"},{"location":"installation-setup/#server","title":"Server","text":"<pre><code>from kade_drive.server import start_server\nstart_server()\n</code></pre>"},{"location":"other-dependencies/","title":"Other Dependencies","text":""},{"location":"other-dependencies/#messagesystem-dependency","title":"MessageSystem Dependency","text":"<p><code>kade_drive</code> relies on the <code>MessageSystem</code> package as a side dependency for communication between new nodes and the network. <code>MessageSystem</code> provides a messaging system using multicast and broadcast, allowing for efficient message exchange.</p> <p>The <code>MessageSystem</code> package is a separate Python package that needs to be installed as a prerequisite for using <code>kade_drive</code>. It handles the low-level communication aspects required for the functioning of <code>kade_drive</code>.</p> <p>To install <code>MessageSystem</code>, you can use pip. Run the following command in your terminal:</p> <pre><code>pip install message-system\n</code></pre>"},{"location":"other-dependencies/#usage","title":"Usage","text":"<p>Once MessageSystem is installed, you can use kade_drive as described in this doc. It will automatically utilize the <code>MessageSystem</code> package for autodiscovery.</p>"},{"location":"recommendations/","title":"Recommendations","text":"<ul> <li> <p>In the event of a node failure in the network, it is recommended to somehow keep the ID of the previous node. If its information is still stored and updated on disk, loading with this ID can be more efficient than starting as a new node and going through the network rebalancing process again.</p> </li> <li> <p>In the original implementation of Kademlia, node communication was done using UDP. However, due to UDP's limitations in handling large amounts of information, it was changed to TCP. Nevertheless, TCP is less efficient for tasks other than storage data transfer. Implementing a dual communication protocol between nodes should improve network performance.</p> </li> <li> <p>Consider changing the hash algorithm used from SHA1 to SHA256, as SHA1 is no longer considered secure and its vulnerabilities facilitate malicious activities.</p> </li> <li> <p>Implement a distributed automatic testing mechanism using techniques like \"chaos testing\" and \"swarm testing\".</p> </li> <li> <p>Implement an authentication system so that only authorized clients can connect to the servers and these only have access to the necessary RPCs.</p> </li> </ul>"},{"location":"replication/","title":"Replication","text":"<p>For the implemented replication algorithm, a thread is used to execute the following steps at a given time interval (i):</p> <ul> <li>Iterate through all keys in the storage whose timestamp is greater than a specified time (t). If the keys have the <code>republish</code> property set to True, the node republishes them.</li> <li>Iterate through all keys in the storage and check how many replicas can be found in the network for each key. If the number of replicas is below the specified replication factor, the node republishes them. Keys that exceed the replication factor are deleted from the farthest nodes until the replication factor is matched.</li> </ul>"},{"location":"why-kademlia/","title":"Why Kademlia?","text":"<p>One of the advantages of Kademlia is the efficiency of its routing algorithm, which enables efficient communication between nodes in the network. The binary prefix of Kademlia IDs ensures that information is routed to the closest node, minimizing the number of hops between nodes and overall network latency.</p> <p>Another advantage of this protocol is its ability to handle node failures and network partitioning. The republishing mechanism in Kademlia ensures that nodes' routing tables stay updated, maintaining connectivity in the network and minimizing data loss as much as possible.</p> <p>Data can be distributed across multiple nodes in the network. Kademlia uses a distributed hash table to keep track of the data's location in the network. This enables nodes to efficiently find the location of the data they need to process or analyze. Additionally, the architecture of Kademlia ensures fault tolerance, meaning that if a node becomes disconnected or fails, the data will still be available on other nodes in the network.</p> <p>This capability of distributed storage and retrieval of data in Kademlia is particularly useful for systems that handle large volumes of data that need to be processed in parallel. By distributing the data among multiple nodes, faster and more efficient processing and analysis can be achieved. Compared to alternatives like Chord, Kademlia is a better choice for applications that require efficient routing and frequent information updates, which is believed to be the main use case for this system after its integration with Autogoal.</p>"},{"location":"why-kademlia/#real-world-examples-of-kademlia-usage","title":"Real-world Examples of Kademlia Usage","text":"<p>Some companies that use this protocol include:</p> <ul> <li>Storj: A cloud storage platform that used a modified version of the protocol in its version 3 for implementing a system with DNS-like capabilities.</li> <li>Ethereum Protocol: The Ethereum protocol uses a slightly modified version of Kademlia, maintaining the XOR-based ID method and K-buckets.</li> <li>The Interplanetary FileSystem (IPFS): In the implementation of IPFS, the NodeID contains a direct mapping to IPFS file hashes. Each node also stores information on where to obtain the file or resource.</li> </ul>"},{"location":"es/","title":"Introducci\u00f3n","text":"<p>Este proyecto utiliza como punto de partida Kademlia. Kademlia es una tabla de hash distribuida que permite que millones de ordenadores se organicen autom\u00e1ticamente en una red, se comuniquen con otros ordenadores de la red y compartan recursos.</p> <p>Se hace uso de varios enfoques e ideas propuestas por Kademlia, muchas de ellas se usan como base para ampliar las funcionalidades del proyecto y otras son utilizadas de la misma manera que Kademlia.</p> <p>Una vez que se tiene un sistema de ficheros se integra a AutoGOAL para resolver problemas de aprendizaje de m\u00e1quina de una manera m\u00e1s eficiente en cuanto a rendimiento, uso de la red y almacenamiento.</p>"},{"location":"es/autodiscovery/","title":"Autodescubrimiento","text":"<p>Para el mecanismo de autodescubrimiento, tambi\u00e9n se crea un hilo (Thread) que emite un latido en las direcciones de broadcast de cada una de las NICs del host, con un identificador de la fuente del broadcast, la IP y el puerto \"dfs ip puerto\". Cada vez que un nuevo nodo se une a la red, escucha los mensajes de difusi\u00f3n (broadcast) para encontrar vecinos. Este mecanismo es accesible desde el cliente, lo que hace que la conexi\u00f3n sea transparente para el usuario.</p>"},{"location":"es/autodiscovery/#desventajas-de-este-enfoque","title":"Desventajas de este enfoque","text":"<p>Una desventaja de este enfoque es que se utiliza el broadcast para el autodescubrimiento, lo que solo es posible en una red local o mediante el uso de una (o varias) PC como puentes entre diferentes subredes. Esto significa que si no existe una forma de conectar las computadoras localmente, no podr\u00e1n descubrirse entre s\u00ed. Sin embargo, dado que el sistema est\u00e1 dise\u00f1ado para la conexi\u00f3n de diferentes estaciones de trabajo, se asume que estar\u00e1n en la misma red local, lo que hace factible este enfoque.</p>"},{"location":"es/cap-analysis/","title":"An\u00e1lisis del teorema CAP","text":"<p>El teorema CAP, tambi\u00e9n conocido como el teorema de Brewer, es un concepto fundamental en los sistemas distribuidos que establece que es imposible que un almac\u00e9n de datos distribuido proporcione simult\u00e1neamente consistencia (C), disponibilidad (A) y tolerancia a particiones (P).</p> <ul> <li> <p>Consistencia (C): La consistencia se refiere a que todos los nodos en un sistema distribuido tengan la misma visi\u00f3n de los datos al mismo tiempo. En este sistema, lograr una consistencia estricta en todos los nodos no es una prioridad. En cambio, se garantiza la consistencia eventual, lo que significa que con el tiempo, todos los nodos converger\u00e1n al mismo estado. Los nodos intercambian peri\u00f3dicamente informaci\u00f3n y actualizan sus tablas de enrutamiento para lograr esta convergencia.</p> </li> <li> <p>Disponibilidad (A): La disponibilidad implica que el sistema permanezca receptivo y accesible para los usuarios incluso en presencia de fallos de nodos o particiones de red. El sistema prioriza la disponibilidad asegurando que los nodos puedan seguir operando y proporcionando servicios incluso cuando algunos nodos est\u00e9n no disponibles o inalcanzables. Esto se logra a trav\u00e9s de la redundancia y la replicaci\u00f3n de datos, donde se almacenan m\u00faltiples copias de datos en diferentes nodos.</p> </li> <li> <p>Tolerancia a particiones (P): La tolerancia a particiones se refiere a la capacidad del sistema para continuar funcionando y proporcionar servicios a pesar de particiones o fallos en la red. El sistema est\u00e1 dise\u00f1ado para ser tolerante a particiones, lo que permite que la red siga operando y mantenga su funcionalidad incluso cuando los nodos est\u00e9n temporalmente desconectados o aislados debido a problemas de red.</p> </li> </ul> <p>Todos estos puntos se analizan asumiendo que no se sobrepasa la capacidad de recuperaci\u00f3n del sistema. Si se superase dicho punto, el sistema comenzar\u00eda a verse afectada la disponibilidad. Pero esto solo ocurrir\u00eda en caso de un cierto n\u00famero de fallos simult\u00e1neos, puesto que el sistema es capaz de recuperarse de fallos eventuales sin ning\u00fan problema.</p>"},{"location":"es/client/","title":"Cliente","text":"<p>Se desarroll\u00f3 un cliente que cuenta con las siguientes caracter\u00edsticas:</p> <ul> <li>Puede recibir puntos de entrada a la red, conocidos como bootstrap nodes, para establecer una conexi\u00f3n directa con el sistema de archivos.</li> <li>Posee un mecanismo de autodescubrimiento que recibe mensajes de difusi\u00f3n (broadcast) en todas las NICs de la PC. Si no recibe ning\u00fan bootstrap node o no puede establecer conexi\u00f3n con ninguno, puede utilizar esta funcionalidad para encontrar autom\u00e1ticamente alg\u00fan nodo.</li> <li>Tiene la capacidad de descubrir otros nodos autom\u00e1ticamente al conectarse con un nodo existente y obtener informaci\u00f3n sobre sus vecinos.</li> <li>Maneja errores relacionados con la inestabilidad de la red o ca\u00eddas inesperadas de un nodo. Permite al usuario establecer el n\u00famero de reintentos cuando se pierde la conexi\u00f3n con un nodo. Adem\u00e1s, posee una cola con los nodos conocidos. Cuando se agotan los reintentos de conexi\u00f3n con un nodo, se elimina de la cola y se pasa al siguiente. Tambi\u00e9n es posible especificar si se desea utilizar el mecanismo de autodescubrimiento cuando la cola est\u00e1 vac\u00eda.</li> </ul>"},{"location":"es/description/","title":"Descripci\u00f3n del sistema","text":"<p>El sistema utiliza un enfoque similar a Kademlia, donde las claves se almacenan como valores de 160 bits. Cada nodo en la red tiene un ID \u00fanico y los pares (clave, valor) se almacenan en nodos con ID \"cercanos\" utilizando la m\u00e9trica XOR propuesta por Kademlia.</p> <p>El sistema trata los nodos como hojas de un \u00e1rbol binario, donde la posici\u00f3n de cada nodo se determina por el prefijo \u00fanico m\u00e1s corto de su ID. Estos nodos almacenan informaci\u00f3n de contacto para enrutar los mensajes de consulta.</p> <p>Se utiliza el protocolo de Kademlia, que incluye los RPCs (Remote Procedure Calls) PING, STORE, FINDNODE y FIND-VALUE. Adem\u00e1s, se implementan otros RPCs como CONTAINS, BOOTSTRAPPABLE-NEIGHBOR, GET, GET-FILE-CHUNKS, UPLOAD-FILE, SET-KEY, CHECK-IF-NEW-VALUE-EXIST, GET-FILE-CHUNK-VALUE, GET-FILE-CHUNK-LOCATION, FIND-CHUNK-LOCATION y FIND-NEIGHBORS, etc. A continuaci\u00f3n se explica el funcionamiento de los mismos.</p> <ul> <li><code>GET</code> : Obtiene la informaci\u00f3n identificada con la llave de un archivo, devuelve una lista con las llaves de cada chunk.</li> <li><code>UPLOAD-FILE</code> : Sube el archivo al sistema de ficheros, lo divide en chunks y guarda la metadata del archivo con la forma de unificar todos los chunks.</li> <li><code>GET-FIND-CHUNK-VALUE</code> : Devuelve el valor asociado al chunk.</li> <li><code>GET-FIND-CHUNK-LOCATION</code> : Recibe la llave de un chunk y devuelvemuna tupla (ip, puerto) donde se encuentra.</li> <li><code>FIND-NEIGHBORS</code> : Devuelve los vecinos al nodo acorde a la cercan\u00eda seg\u00fan la m\u00e9trica XOR. Adem\u00e1s, los servidores utilizan los siguientes RPCs:</li> <li><code>CONTAINS</code> : Detecta si una llave est\u00e1 en un nodo, esto se utiliza tanto para la replicaci\u00f3n de la informaci\u00f3n como para encontrar si un nodo tiene la informaci\u00f3n que se desea.</li> <li><code>GET-FILE-CHUNKS</code> : Obtiene la lista de ubicaciones de los chunks de la informaci\u00f3n.</li> <li><code>SET-KEY</code>: Guarda un par (llave, valor) en el sistema</li> <li><code>DELETE</code>: Elimina un fichero de la red</li> <li><code>GET-ALL-FILE-NAMES</code>: Realiza la funci\u00f3n del comando <code>ls</code> en linux, devuelve una lista con todos los ficheros en el sistema(todas las llaves de metadata)</li> </ul> <p>El sistema tambi\u00e9n cuenta con un m\u00f3dulo de persistencia llamado <code>PersistentStorage</code>, que maneja la escritura y lectura de datos. Utiliza las siguientes rutas:</p> <ul> <li><code>static/metadata</code>: se almacenan los nombres de los archivos que representan los hash de los datos divididos en chunks de m\u00e1ximo 1000kb. Estos archivos contienen listas de Python guardadas con pickle, que contienen los hashes de cada chunk obtenido al dividir los datos.</li> <li><code>static/keys</code>: se almacenan los nombres de los archivos que representan los hashes de los datos almacenados, ya sea de datos completos o de chunks. Estos archivos contienen los hashes correspondientes en bytes.</li> <li><code>static/values</code>: se almacenan los nombres de los archivos que representan los hashes de los chunks almacenados, excluyendo los hashes de datos sin dividir.</li> <li><code>timestamps</code>: se almacenan los nombres de los archivos que representan los hashes de los datos almacenados, al igual que en la ruta de <code>keys</code>, pero contienen un diccionario de Python guardado con pickle. Este diccionario tiene como claves <code>date</code> con el valor <code>datetime.now()</code>, <code>republish</code> con un valor booleano. Esta informaci\u00f3n se utiliza para llevar un registro de la \u00faltima vez que se accedi\u00f3 a una llave y para determinar si es necesario republicar la informaci\u00f3n para mantener la propiedad del sistema de que los nodos mas cercanos a ella sean los q la tengan.</li> </ul> <p>Cuando se recibe un par <code>(clave, valor)</code> en formato bytes, el m\u00f3dulo <code>PersistentStorage</code> codifica la clave utilizando <code>base64.urlsafe_b64encode</code> para obtener un string que se puede utilizar como nombre de archivo. Luego, se escribe un archivo con ese nombre en las rutas de <code>keys</code> y <code>values</code>, donde se guarda la clave en bytes en el archivo de keys y el valor en bytes en el archivo de values. En el caso de que el par a almacenar sea metadata, el valor en bytes se escribe en la ruta de metadata. En ambos casos, tambi\u00e9n se crea un archivo en la ruta de timestamps con el nombre correspondiente.</p> <p>Antes de escribir un valor o metadata, se crea un diccionario con las llaves:</p> <ul> <li><code>integrity</code>: Se utiliza para saber si el fichero esta corrupto, en caso de ser as\u00ed, no se devuelve nunca.</li> <li><code>value</code>: Valor a escribir</li> <li><code>integrity_date</code>: Momento en q se setea por primera vez la integridad. En caso de que pase un tiempo determinado desde esta fecha e integrity este en False, se borra autom\u00e1ticamente el fichero.</li> <li><code>key_name</code>: String de la llave q representa un fichero, se utiliza para poder devolver los nombres originales de los ficheros si se ejecuta el comando <code>ls</code> en el cliente</li> <li><code>last_write</code>: Momento en el que se escribi\u00f3 por \u00faltima vez el fichero. Se utiliza para manejar los casos en que luego de una partici\u00f3n, si se escribi\u00f3 en las dos particiones, valores con iguales llaves, al recuperarse la red, se mantengan los valores escritos m\u00e1s recientemente.</li> </ul> <p>El enrutamiento en el sistema utiliza una estructura de tablas de enrutamiento similar a Kademlia. La tabla de enrutamiento es un \u00e1rbol binario compuesto por k-buckets. Cada k-bucket contiene nodos con un prefijo com\u00fan en sus ID, y el prefijo determina la posici\u00f3n del k-bucket en el \u00e1rbol binario. Los k-buckets cubren diferentes partes del espacio de ID y, juntos, cubren todo el espacio de ID de 160 bits sin solaparse. Los nodos se asignan din\u00e1micamente seg\u00fan sea necesario.</p> <p>Para garantizar la replicaci\u00f3n de datos, los nodos deben republicar peri\u00f3dicamente las claves. Esto se debe a que algunos de los k nodos que inicialmente obtienen un par clave-valor pueden abandonar la red, y nuevos nodos con IDs m\u00e1s cercanos a la clave pueden unirse a la red. Por lo tanto, los nodos que almacenan el par clave-valor deben republicarlo para asegurarse de que est\u00e9 disponible en los k nodos m\u00e1s cercanos a la clave.</p> <p>Cuando un cliente solicita un determinado valor al sistema, se le devuelve una lista de las ubicaciones de los distintos chunks de datos, que pueden estar en diferentes PCs. Luego, el cliente establece una conexi\u00f3n con la PC m\u00e1s cercana a la informaci\u00f3n para obtener los datos y unificarlos. Una vez que un nodo env\u00eda informaci\u00f3n, marca el archivo como pendiente de republicar y actualiza su timestamp, informando a los vecinos que tambi\u00e9n deben replicar la informaci\u00f3n.</p> <p>Cuando un servidor descubre un nuevo nodo, para cada clave almacenada, el sistema recupera los k nodos m\u00e1s cercanos. Si el nuevo nodo est\u00e1 m\u00e1s cerca que el nodo m\u00e1s alejado de esa lista, y el nodo para este servidor est\u00e1 m\u00e1s cerca que el nodo m\u00e1s cercano de esa lista, entonces el par clave/valor se almacena en el nuevo nodo.</p> <p>Cuando un servidor inicia una conexi\u00f3n nueva con un cliente, inicia un Thread nuevo para manejar dicha conexi\u00f3n.</p> <p>Cuando el cliente comienza a subir un archivo, los datos se dividen en chunks de m\u00e1ximo 1000kb y se almacenan en el sistema de ficheros con <code>integrity</code> en <code>False</code>. Luego, se almacena la metadata del archivo, que contiene la informaci\u00f3n necesaria para unificar los chunks y reconstruir el archivo original tambi\u00e9n con <code>integrity</code> en <code>False</code>. Una vez que se confirma que se pudieron almacenar todos los chunks y la metadata, comienza a confirmarse la integridad de cada chunk en la red y despu\u00e9s la de la metadata. En caso de ocurrir alg\u00fan error en cualquier parte del proceso, se borra todo lo almacenado, haciendo el efecto de un rollback.</p>"},{"location":"es/examples/","title":"Ejemplos","text":""},{"location":"es/examples/#guardar-un-valor","title":"Guardar un valor","text":""},{"location":"es/examples/#nota-asegurarse-de-que-existe-al-menos-un-servidor-en-la-red","title":"Nota: Asegurarse de que existe al menos un servidor en la red","text":"<pre><code>from kade_drive.cli import ClientSession\nclient = ClientSession()\nclient.connect()\nresponse, _ = client.put(4, 5)\nvalue, _ = client.get(4)\nassert value == 5\n</code></pre>"},{"location":"es/examples/#guardar-un-dataframe","title":"Guardar un Dataframe","text":"<pre><code>    from sklearn.datasets import load_diabetes\nimport pandas as pd\nimport pickle\nfrom client import ClientSession\nfrom time import sleep\nX, y = load_diabetes(return_X_y=True)\nX = pd.DataFrame(X)\ny = pd.DataFrame(y)\nX_pickle = pickle.dumps(X)\ninitial_bootstrap_nodes = []\nclient_session = ClientSession(initial_bootstrap_nodes)\nclient_session.connect(use_broadcast_if_needed=True)\nresponse, _ = client_session.put(\"dataset_diabetes\", X_pickle)\nsleep(5)\nvalue_getted, _ = client_session.get(\"dataset_diabetes\")\nassert X.equals(value_getted)\n</code></pre>"},{"location":"es/fault-tolerance/","title":"Tolerancia a Fallas","text":"<p>Los nodos mantienen informaci\u00f3n sobre otros nodos cercanos en la red, almacenando estos nodos en su tabla de enrutamiento. Cada nodo tiene una lista de <code>k-buckets</code> que contienen los detalles de contacto de otros nodos en la red, clasificados seg\u00fan su proximidad en el espacio de identificaci\u00f3n.</p> <p>Cuando un nodo deja de responder o se desconecta, los otros nodos de la red detectan la falta de respuesta despu\u00e9s de un per\u00edodo de tiempo determinado. En ese momento, se considera que el nodo fallido est\u00e1 inactivo.</p> <p>Cuando un nodo detecta la inactividad de otro nodo, actualiza su tabla de enrutamiento eliminando la entrada del nodo fallido. Adem\u00e1s, el nodo realiza una serie de acciones para mantener la conectividad y la redundancia en la red.</p> <ul> <li> <p>Replicaci\u00f3n de datos: Si el nodo fallido almacenaba datos, otros nodos de la red pueden asumir la responsabilidad de mantener r\u00e9plicas de esos datos. De esta manera, los datos permanecen disponibles incluso si el nodo original se desconecta. Los nodos que asumir\u00e1n esta responsabilidad ser\u00e1n aquellos que est\u00e9n m\u00e1s cercanos utilizando la misma m\u00e9trica de XOR con los IDs para mantener la red lo m\u00e1s optimizada posible.</p> </li> <li> <p>Actualizaci\u00f3n de informaci\u00f3n de enrutamiento: Los nodos que ten\u00edan al nodo fallido en su tabla de enrutamiento actualizan esa entrada elimin\u00e1ndola. De esta manera, los nodos evitan enviar mensajes o realizar acciones hacia un nodo que ya no est\u00e1 disponible. En caso de quedarse sin vecinos un nodo puede ejecutar un broadcast para tratar de descubrir otros nodos en la red.</p> </li> <li> <p>La expiraci\u00f3n de contacto garantiza que la informaci\u00f3n almacenada en la red permanezca accesible incluso cuando los nodos individuales fallan. Al eliminar los nodos inactivos de los buckets, se asegura de que las rutas de enrutamiento se actualicen y se mantengan eficientes.</p> </li> </ul>"},{"location":"es/installation-setup/","title":"Instalaci\u00f3n","text":"<p>Para instalar la biblioteca <code>kade-drive</code>, puedes ejecutar el siguiente comando en la l\u00ednea de comandos:</p> <pre><code>pip install kade-drive\n</code></pre>"},{"location":"es/installation-setup/#server","title":"Server","text":"<pre><code>from kade_drive.server import start_server\nstart_server()\n</code></pre>"},{"location":"es/other-dependencies/","title":"otras-dependencias","text":""},{"location":"es/other-dependencies/#dependencia-de-messagesystem","title":"Dependencia de MessageSystem","text":"<p><code>kade_drive</code> depende del paquete <code>MessageSystem</code> como una dependencia adicional para la comunicaci\u00f3n entre los nuevos nodos y la red. <code>MessageSystem</code> proporciona un sistema de mensajer\u00eda utilizando multicast y broadcast, lo que permite un intercambio eficiente de mensajes.</p> <p>El paquete <code>MessageSystem</code> es un paquete separado de Python que debe ser instalado como requisito previo para usar <code>kade_drive</code>. Maneja los aspectos de comunicaci\u00f3n de bajo nivel necesarios para el funcionamiento de <code>kade_drive</code>.</p> <p>Para instalar <code>MessageSystem</code>, puedes utilizar pip. Ejecuta el siguiente comando en tu terminal:</p> <pre><code>pip install message-system\n</code></pre>"},{"location":"es/other-dependencies/#uso","title":"Uso","text":"<p>Una vez instalado <code>MessageSystem</code>, puedes usar kade_drive como se describe en esta documentaci\u00f3n. Utilizar\u00e1 autom\u00e1ticamente el paquete <code>MessageSystem</code> para la autodetecci\u00f3n de nodos.</p> <p>N\u00f3tese que esto no es necesario si se instala utilizando poetry ya que este se encarga de instalar todas las dependencias</p>"},{"location":"es/other-dependencies/#repositorio","title":"Repositorio","text":"<p>https://github.com/geeksLabTech/message-system</p>"},{"location":"es/recomendations/","title":"Recomendaciones","text":"<ul> <li> <p>Dada la ca\u00edda de un nodo de la red, mantener de alguna manera el ID del nodo anterior. Si su informaci\u00f3n a\u00fan se mantiene en disco y actualizada, cargar con este ID puede ser m\u00e1s eficiente que iniciar como un nodo nuevo y comenzar nuevamente el proceso de balanceo de la red.</p> </li> <li> <p>En la implementaci\u00f3n original de Kademlia, la comunicaci\u00f3n entre nodos se realiza mediante UDP. Sin embargo, debido a las limitaciones de UDP para manejar grandes cantidades de informaci\u00f3n, se cambi\u00f3 a TCP. No obstante, TCP es menos eficiente para tareas que no sean transferencia de datos de almacenamiento. Implementar un doble protocolo de comunicaci\u00f3n entre los nodos deber\u00eda representar una mejora en el rendimiento de la red.</p> </li> <li> <p>Cambiar el algoritmo de hash utilizado de SHA1 a SHA256, dado que SHA1 ya no es considerado seguro y sus vulnerabilidades facilitan actividades maliciosas.</p> </li> <li> <p>Implementar un mecanismo de testing autom\u00e1tico distribuido utilizando t\u00e9cnicas como \"chaos testing\" y \"swarm testing\".</p> </li> <li> <p>Implementar un sistema de autenticaci\u00f3n para que a los servidores solo se puedan conectar clientes autorizados y estos solamente tengan acceso a los RPC necesarios</p> </li> </ul>"},{"location":"es/replication/","title":"Replicaci\u00f3n","text":"<p>Para el algoritmo de replicaci\u00f3n implementado se utiliza un hilo (Thread) que se ejecuta en un intervalo de tiempo (i) y consiste en lo siguiente:</p> <ul> <li>Recorrer todas las llaves en el almacenamiento (storage) cuyo timestamp sea mayor que un tiempo (t). Aquellas llaves que tengan la propiedad <code>republish</code> en True son republicadas por el nodo.</li> <li>Recorrer todas las llaves en el almacenamiento y verificar en la red cu\u00e1ntas r\u00e9plicas se pueden encontrar de cada una. Aquellas llaves que est\u00e9n por debajo del factor de replicaci\u00f3n especificado son republicadas por el nodo. Aquellas que est\u00e9n por encima, se borran de los nodos m\u00e1s lejanos hasta igualar el factor de replicaci\u00f3n especificado. </li> </ul>"},{"location":"es/why-kademlia/","title":"\u00bfPor qu\u00e9 Kademlia?","text":"<p>Una de las ventajas de Kademlia es la eficiencia de su algoritmo de enrutamiento, lo que permite una comunicaci\u00f3n eficiente entre los nodos de la red. El prefijo binario de los ID de Kademlia hace que se env\u00ede la informaci\u00f3n hacia el nodo m\u00e1s cercano, minimizando la cantidad de saltos entre nodos y la latencia de la red en general.</p> <p>Otra de las ventajas de este protocolo es la capacidad de manejar fallos de nodos y particionado de la red. El mecanismo de republicaci\u00f3n de Kademlia garantiza que se mantengan actualizadas las tablas de ruta de los nodos, manteniendo as\u00ed la conectividad en la red y evitando la p\u00e9rdida de datos dentro de lo posible.</p> <p>Los datos pueden estar dispersos en m\u00faltiples nodos de la red. Kademlia utiliza una tabla hash distribuida para mantener un registro de la ubicaci\u00f3n de los datos en la red. Esto permite que los nodos encuentren de manera eficiente la ubicaci\u00f3n de los datos que necesitan procesar o analizar. Adem\u00e1s, la arquitectura de Kademlia garantiza la tolerancia a fallos, lo que significa que si un nodo se desconecta o falla, los datos todav\u00eda estar\u00e1n disponibles en otros nodos de la red.</p> <p>Esta capacidad de almacenamiento y recuperaci\u00f3n distribuida de datos de Kademlia es especialmente \u00fatil para los sistemas que suelen manejar grandes vol\u00famenes de datos que necesitan ser procesados en paralelo. Al distribuir los datos entre varios nodos, se puede lograr un procesamiento y an\u00e1lisis m\u00e1s r\u00e1pido y eficiente. En comparaci\u00f3n con alternativas como Chord, Kademlia es una mejor opci\u00f3n para aplicaciones que requieran un enrutamiento eficiente y frecuentes actualizaciones de informaci\u00f3n, lo cual se cree que ser\u00e1 el principal caso de uso de este sistema posterior a su integraci\u00f3n con Autogoal.</p>"},{"location":"es/why-kademlia/#ejemplos-reales-del-uso-de-kademlia","title":"Ejemplos reales del uso de Kademlia","text":"<p>Entre las compa\u00f1\u00edas que utilizan este protocolo se encuentran:</p> <ul> <li>Storj: Plataforma de almacenamiento en la nube que en su versi\u00f3n 3 utiliz\u00f3 una versi\u00f3n modificada del protocolo para la implementaci\u00f3n de un sistema con capacidades similares a un DNS.</li> <li>Protocolo Ethereum: El protocolo Ethereum utiliza una versi\u00f3n ligeramente modificada de Kademlia, manteniendo el m\u00e9todo de identificaci\u00f3n con XOR y los K-buckets.</li> <li>The Interplanetary FileSystem: En la implementaci\u00f3n de IPFS, el NodeID contiene un mapa directo hacia los hash de archivos de IPFS. Cada nodo tambi\u00e9n almacena informaci\u00f3n sobre d\u00f3nde obtener el archivo o recurso.</li> </ul>"}]}