{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"01 - Introduction","text":"<p>This project builds upon the foundation of Kademlia. Kademlia is a distributed hash table that enables millions of computers to self-organize into a network, communicate with each other, and share resources.</p> <p>Several approaches and ideas proposed by Kademlia are utilized in this project, serving as the basis to expand the functionalities of the system, while others are employed in a similar manner as Kademlia.</p> <p>Once the file system is established, it is integrated with AutoGOAL to tackle machine learning problems in a more efficient manner in terms of performance, network utilization, and storage.</p>"},{"location":"autodiscovery/","title":"06 - Autodiscovery","text":"<p>For the autodiscovery mechanism, a <code>Thread</code> is also created that sends a heartbeat to the broadcast addresses of each NIC (Network Interface Card) on the host, with a broadcast source identifier and the IP and port \"<code>dfs ip port</code>\". Whenever a new node joins the network, it listens to the broadcasts to discover neighbors. This mechanism is accessible from the client, making the connection transparent to the user.</p>"},{"location":"autodiscovery/#disadvantages-of-this-approach","title":"Disadvantages of this approach","text":"<p>Since broadcast is used for autodiscovery, it is only possible within a local network or by using one (or multiple) PCs as a bridge between different subnets. If, for some reason, there is no way to connect computers locally, they will not be able to discover each other. However, since the system is designed for connecting different workstations, it is assumed that they will be on the same local network, making this approach feasible.</p>"},{"location":"cap-analysis/","title":"09 - CAP Analysis","text":"<p>The CAP theorem, also known as Brewer's theorem, is a fundamental concept in distributed systems that states that it is impossible for a distributed data store to simultaneously provide consistency (C), availability (A), and partition tolerance (P).</p> <ul> <li> <p>Consistency (C): Consistency refers to the requirement that all nodes in a distributed system have the same view of the data at the same time. In this system, achieving strict consistency across all nodes is not a priority. Instead, eventual consistency is guaranteed, which means that over time, all nodes will converge to the same state. Nodes periodically exchange information and update their routing tables to achieve this convergence.</p> </li> <li> <p>Availability (A): Availability implies that the system remains responsive and accessible to users even in the presence of node failures or network partitions. The system prioritizes availability by ensuring that nodes can continue to operate and provide services even when some nodes are unavailable or unreachable. This is achieved through redundancy and data replication, where multiple copies of data are stored on different nodes.</p> </li> <li> <p>Partition Tolerance (P): Partition tolerance refers to the system's ability to continue functioning and providing services despite network partitions or failures. The system is designed to be partition-tolerant, allowing the network to operate and maintain its functionality even when nodes are temporarily disconnected or isolated due to network issues.</p> </li> </ul> <p>All these points are analyzed assuming that the system's recovery capacity is not exceeded. If that threshold were to be surpassed, the system's availability would be affected. However, this would only occur in the case of simultaneous failures, as the system is capable of recovering from occasional failures without any issues.</p>"},{"location":"client/","title":"05 - Client","text":"<p>A client has been developed with the following features:</p> <ul> <li>It can receive entry points to the network, known as bootstrap nodes, for a direct connection to the file system.</li> <li>It has an autodiscovery mechanism that listens to broadcasts on all NICs (Network Interface Cards) of the PC. If it doesn't receive any bootstrap node or is unable to connect to any, it can use this functionality to automatically discover a node.</li> <li>It has the ability to discover other nodes automatically by connecting to a node and exploring its neighbors.</li> <li>It handles errors related to network instability or unexpected node failures. The user can specify the number of connection retries when a connection to a node is lost. It maintains a queue of known nodes, and when the maximum number of connection retries is reached for a node, it is removed from the queue and the next node is used. It is also possible to specify whether the autodiscovery mechanism should be used when the queue becomes empty.</li> </ul>"},{"location":"description/","title":"03 - Description","text":"<p>The system follows the approach of many distributed hash table systems, where keys are stored as 160-bit values. Each participating computer in the network has an ID, and the (key, value) pairs are stored on nodes with \"close\" IDs, using the XOR metric proposed by Kademlia as the distance metric.</p> <p>Similar to Kademlia, nodes are treated as leaves in a binary tree, where the position of each node is determined by the \"shortest unique prefix\" of its ID. These nodes store contact information for each other to route query messages.</p> <p>The system is based on the Kademlia protocol, which guarantees that each node knows at least one node from each of its subtrees (if they contain any nodes). With this guarantee, any node can locate another node by its ID.</p> <p>There is a persistence module called PersistentStorage that operates as follows: It uses the following paths:</p> <ul> <li>static/metadata: The file names in this path represent the hash of data that has been divided into multiple chunks with a maximum size of 1000kb. Each file contains a Python list stored with pickle, which holds the hashes of each chunk obtained from the data.</li> <li>static/keys: The file names in this path represent the hashes of stored data, whether it corresponds to a complete data or a chunk of it. Each file contains the corresponding hashes in bytes.</li> <li>static/values: The file names in this path represent the hashes of all the chunks that have been stored, excluding hashes of unchunked data.</li> <li>timestamps: The file names in this path represent the hashes of stored data, similar to the keys path. However, each file contains a Python dictionary stored with pickle, which has keys \"date\" with the value \"datetime.now()\" and \"republish\" with a boolean value. This is used to keep track of the last access time of a key and to determine if the node holding the key needs to republish it because it is frequently accessed.</li> </ul> <p>When receiving a (key, value) pair, both of type bytes, the key is encoded using base64.urlsafeb64encode to obtain a string that can be used as a file name. A file is then written with that name in the keys and values paths. In the keys file, the key is written as bytes, and in the values file, the value is written as bytes. If the pair to be stored is metadata, the file containing the value as bytes is written in the metadata path. In both cases, a file is created in the timestamps path with the corresponding name.</p> <p>The Kademlia protocol includes four RPCs: PING, STORE, FINDNODE, and FINDVALUE. In addition to using these four RPCs, this proposal implements the following:</p> <ul> <li>CONTAINS: Determines if a key exists in a node. This is used for both information replication and to find out if a node has the desired information.</li> <li>BOOTSTRAPPABLE-NEIGHBOR</li> <li>GET: Retrieves the information identified by the key of a chunk.</li> <li>GET-FILE-CHUNKS: Retrieves the list of locations of the chunks that make up the information.</li> <li>UPLOAD-FILE: Uploads a file to the file system, divides it into chunks, and stores the metadata of the file to unify all the files.</li> <li>SET-KEY</li> <li>FIND-NEIGHBORS</li> </ul> <p>The same routing table structure as Kademlia is used. The routing table is a binary tree whose leaves are k-buckets. Each k-bucket contains nodes with a common prefix in their IDs. The prefix represents the position of the k-bucket in the binary tree, so each k-bucket covers a portion of the 160-bit ID space, and together, the k-buckets cover the entire ID space without overlap. The nodes of the routing tree are dynamically assigned as needed.</p> <p>To ensure proper data replication, nodes need to periodically republish keys. Otherwise, two phenomena can cause valid key searches to fail. First, some of the k nodes that initially obtain a key-value pair when it is published may leave the network. Second, new nodes may join the network with IDs closer to a published key than the nodes on which the key-value pair was originally published. In both cases, nodes with the key-value pair need to republish to ensure it is available on the k nodes closest to the key.</p> <p>When a client requests a specific value from the system, it receives a list of locations of the different data chunks (which may not be on the same PC). A connection is established with the PC closest to the information to avoid unnecessary network congestion. The client then combines the data and returns the stored value. Once a node sends certain information, that file is marked as pending for republication, and its timestamp is updated. This ensures that each of the neighboring nodes that need to replicate the information is informed that it has been accessed and should not be deleted.</p>"},{"location":"examples/","title":"1.2 - Examples","text":""},{"location":"examples/#store-a-value-in-the-server","title":"Store a Value in the server","text":""},{"location":"examples/#note-make-sure-that-there-exist-at-least-a-server-in-the-local-network","title":"Note: Make sure that there exist at least a server in the local network","text":"<pre><code>from kade_drive.cli import ClientSession\nclient = ClientSession()\nclient.connect()\nclient.put(4, 5)\nvalue = client.get(4)\nassert value == 5\n</code></pre>"},{"location":"examples/#storing-a-dataframe","title":"Storing a Dataframe","text":"<pre><code>from sklearn.datasets import load_diabetes\nimport pandas as pd\nimport pickle\nfrom client import ClientSession\nfrom time import sleep\nX, y = load_diabetes(return_X_y=True)\nX = pd.DataFrame(X)\ny = pd.DataFrame(y)\nX_pickle = pickle.dumps(X)\ninitial_bootstrap_nodes = []\nclient_session = ClientSession(initial_bootstrap_nodes)\nclient_session.connect(use_broadcast_if_needed=True)\nclient_session.put(\"dataset_diabetes\", X_pickle)\nsleep(5)\nvalue_getted = client_session.get(\"dataset_diabetes\")\nassert X.equals(value_getted)\n</code></pre>"},{"location":"fault-tolerance/","title":"08 - Fault Tolerance","text":"<p>Nodes maintain information about other nodes in the network by storing them in their routing table. Each node has a list of \"k-buckets\" that contain the contact details of other nodes in the network, classified based on their proximity in the ID space.</p> <p>When a node becomes unresponsive or disconnects, other nodes in the network detect the lack of response after a certain period of time. At that point, the failed node is considered inactive.</p> <p>When a node detects the inactivity of another node, it updates its routing table by removing the entry of the failed node. Additionally, the node performs a series of actions to maintain connectivity and redundancy in the network.</p> <ul> <li> <p>Data Replication: If the failed node was storing data, other nodes in the network can take the responsibility of maintaining replicas of that data. This ensures that the data remains available even if the original node disconnects. The nodes that will assume this responsibility are those that are closest using the same XOR metric with IDs to keep the network as optimized as possible.</p> </li> <li> <p>Responsibility Transfer: If the failed node was responsible for certain identifiers in the network, other nodes take over the responsibility of handling them. This ensures that access to the data or resources is not lost.</p> </li> <li> <p>Routing Information Update: Nodes that had the failed node in their routing table update that entry by removing it. This way, nodes avoid sending messages or performing actions towards a node that is no longer available.</p> </li> </ul> <p>The expiration of contact ensures that the information stored in the network remains accessible even when individual nodes fail. By removing inactive nodes from the buckets, it ensures that routing paths are updated and kept efficient.</p>"},{"location":"installation-setup/","title":"1.1 - Installation","text":"<pre><code>pip install kade-drive\n</code></pre>"},{"location":"installation-setup/#running-a-server-server","title":"Running a server Server","text":"<pre><code>from kade_drive.server import start_server\nstart_server()\n</code></pre>"},{"location":"persistence/","title":"04 - Persistence","text":"<p>By default, the information in the system is configured to persist for 1 week if it is not accessed. For demonstration purposes, a 2-minute window is used to evaluate the correct functioning of the system. In a production environment, the time window for data deletion should be carefully analyzed. It is important to note that this file system is designed for interaction among the different nodes in the network and not as a persistent storage system. Therefore, information that is not being used is periodically deleted to ensure that new information can be stored for training algorithms. This mechanism is implemented through a dedicated thread that checks the timestamps of the files and deletes those that fall outside the predefined time window.</p>"},{"location":"recommendations/","title":"10 - Recommendations","text":"<ul> <li> <p>In the event of a node failure, it is recommended to keep track of the ID of the previous node. If its information is still available on disk and up to date, loading it with this ID can be more efficient than starting as a new node and going through the network rebalancing process again.</p> </li> <li> <p>In the original implementation of Kademlia, node communication was done using UDP. However, UDP has limitations in handling large amounts of data. It was changed to TCP, which is less efficient for tasks other than data storage transfer. Implementing a dual communication protocol between nodes could improve network performance.</p> </li> </ul>"},{"location":"replication/","title":"08 - Replication","text":"<p>The implemented replication algorithm uses a Thread that runs at a specified time interval (i) and performs the following steps:</p> <ol> <li> <p>Iterate through all the keys in the storage whose timestamp is greater than a given time (t). Republish the keys that have the \"republish\" flag set to True.</p> </li> <li> <p>Iterate through all the keys in the storage and check how many replicas of each key can be found in the network. If the number of replicas is below the specified replication factor, the node republishes those keys.</p> </li> </ol>"},{"location":"why-kademlia/","title":"02 - Why Kademlia?","text":"<p>One of the advantages of Kademlia is the efficiency of its routing algorithm, which allows for efficient communication between nodes in the network. The binary prefix of Kademlia IDs ensures that information is sent to the closest node, minimizing the number of hops and network latency overall.</p> <p>Another advantage of this protocol is its ability to handle node failures and network partitioning. Kademlia's refreshing mechanism ensures that node routing tables stay up to date, maintaining network connectivity and minimizing data loss as much as possible.</p> <p>Data can be distributed across multiple nodes in the network. Kademlia uses a distributed hash table to keep track of data locations in the network. This allows nodes to efficiently find the location of data they need to process or analyze. Additionally, Kademlia's architecture ensures fault tolerance, meaning that if a node disconnects or fails, the data will still be available on other nodes in the network. This distributed storage and retrieval capability of Kademlia is particularly useful for systems that handle large volumes of data requiring parallel processing. By distributing the data among multiple nodes, faster and more efficient processing and analysis can be achieved.</p>"},{"location":"why-kademlia/#real-world-examples-of-kademlia-usage","title":"Real-World Examples of Kademlia Usage","text":"<p>Some companies that utilize this protocol include:</p> <ul> <li>Storj: A cloud storage platform that used a modified version of the Kademlia protocol in its version 3 for implementing a system with DNS-like capabilities.</li> <li>Ethereum Protocol: The Ethereum protocol uses a slightly modified version of Kademlia, maintaining the XOR-based ID identification and K-buckets.</li> <li>The Interplanetary FileSystem (IPFS): In the implementation of IPFS, the NodeID contains a direct map to IPFS file hashes. Each node also stores information on where to obtain the file or resource.</li> </ul>"},{"location":"es/","title":"Introducci\u00f3n","text":"<p>Este proyecto utiliza como punto de partida Kademlia. Kademlia es una tabla de hash distribuida, que permite que millones de ordenadores se organicen autom\u00e1ticamente en una red, se comuniquen con otros ordenadores de la red y compartan recursos.</p> <p>Se hace uso de varios enfoques e ideas propuestas por Kademlia, muchas de ellas se usan como base para ampliar las funcionalidades del proyecto y otras son utilizadas de la misma manera que Kademlia.</p> <p>Una vez que se tiene un sistema de ficheros se integra a AutoGOAL para resolver problemas de aprendizaje de m\u00e1quina de una manera m\u00e1s eficiente en cuanto a rendimiento, uso de la red y almacenamiento.</p>"},{"location":"es/analisis-cap/","title":"An\u00e1lisis del teorema CAP","text":"<p>El teorema CAP, tambi\u00b4en conocido como el teorema de Brewer, es un concepto fundamental en los sistemas distribuidos que establece que es imposible que un almac\u00b4en de datos distribuido proporcione simult\u00b4aneamente consistencia (C), disponibilidad (A) y tolerancia a particiones (P).</p> <ul> <li> <p>Consistencia (C): La consistencia se refiere a que todos los nodos en un sistema distribuido tengan la misma visi\u00b4on de los datos al mismo tiempo. En este sistema, lograr una consistencia estricta en todos los nodos no es una prioridad. En cambio, se garantiza en la consistencia eventual, lo que significa que con el tiempo, todos los nodos converger\u00b4an al mismo estado. Los nodos intercambian peri\u00b4odicamente informaci\u00b4on y actualizan sus tablas de enrutamiento para lograr esta convergencia.</p> </li> <li> <p>Disponibilidad (A): La disponibilidad implica que el sistema permanezca receptivo y accesible para los usuarios incluso en presencia de fallos de nodos o particiones de red. EL sistema prioriza la disponibilidad asegurando que los nodos puedan seguir operando y proporcionando servicios incluso cuando algunos nodos est\u00b4en no disponibles o inalcanzables. Esto se logra a trav\u00b4es de la redundancia y la replicaci\u00b4on de datos, donde se almacenan m\u00b4ultiples copias de datos en diferentes nodos.</p> </li> <li> <p>Tolerancia a particiones (P): La tolerancia a particiones se refiere a la capacidad del sistema para continuar funcionando y proporcionar servicios a pesar de particiones o fallos en la red. El sistema est\u00b4a dise\u02dcnado para ser tolerante a particiones, lo que permite que la red siga operando y mantenga su funcionalidad incluso cuando los nodos est\u00b4en temporalmente desconectados o aislados debido a problemas de red.</p> </li> </ul> <p>Todos estos puntos se analizan asumiendo que no se sobrepasa la capacidad de recuperaci\u00b4on del sistema, si se superase dicho punto el sistema comenzar\u00b4a a ver afectada la disponibilidad. Pero esto solo Ocurrir\u00b4\u0131a en caso de un cierto numero fallos simult\u00b4aneos, puesto que el sistema es capaz de recuperarse de fallos eventuales sin ning\u00b4un problema.</p>"},{"location":"es/autodescubrimiento/","title":"Autodescubrimiento","text":"<p>Para el mecanismo de auto descubrimiento se crea tambi\u00b4en un Thread que emite un latido en las direcciones de broadcast de cada una de las NICs del host, con un identificador de la fuente del broadcast e ip y puerto \u201ddfs ip puerto\u201d. Cada vez que un nuevo nodo entra a la red escucha los broadcast para encontrar vecinos. El mecanismo es accesible desde el cliente, haciendo transparente la conexi\u00b4on para el usuario.</p>"},{"location":"es/autodescubrimiento/#desventajas-de-este-enfoque","title":"Desventajas de este enfoque","text":"<p>Como se utiliza broadcast para el autodescubrimiento esto solo es posible en una red local, o utilizando una (o varias) PC como puente entre distintas subredes, haciendo que si por alguna raz\u00b4on no existiese una forma de conectar localmente las computadoras, estas no ser\u00b4an capaces de encontrarse, no obstante como el sistema est\u00b4a pensado para conexi\u00b4on de distintas estaciones de trabajo se asume que estas estar\u00b4an en la misma red local haciendo factible este enfoque.</p>"},{"location":"es/cliente/","title":"Cliente","text":"<p>Se desarroll\u00f3 un cliente que cuenta con las siguientes caracter\u00edsticas:</p> <ul> <li>Puede recibir puntos de entrada a la red, conocidos como bootstrap nodes para una conexi\u00b4on directa con el sistema de ficheros</li> <li>Posee un mecanismo de auto-descubrimiento recibiendo broadcast por todas las NICs de la pc, si no recibe ning\u00b4un bootstrap node o no es capaz de conectar con ninguno puede usar esta funcionalidad para encontrar alg\u00b4un nodo autom\u00b4aticamente.</li> <li>Tiene la capacidad de al conectar con alg\u00b4un nodo descubir otros nodos a partir de los vecinos de este de manera autom\u00b4atica.</li> <li>Maneja errores relacionados con inestabilidad de la red o ca\u00b4\u0131das inesperadas de un nodo, permitiendo que el usuario establezca el n\u00b4umero de reintentos que se debe hacer cuando se pierde la conexi\u00b4on con un nodo, posee un cola con los nodos conocidos que al agotarse el n\u00b4umero de reintentos de conexi\u00b4on con un nodo, este es removido de la cola y se pasa al siguiente, es posible especificar tambi\u00b4en si se quiere que se utilice el mecanismo de auto-descubribimiento al quedarse la cola vac\u00b4\u0131a.</li> </ul>"},{"location":"es/descripcion/","title":"Descripci\u00f3n del sistema","text":"<p>Al igual que muchos sistemas de tabla de hash distribuida, las llaves se almacenan en 160 bits. Cada uno de los ordenadores participantes en la red tiene un ID, y los pares (llave, valor) son almacenados en nodos con ID \"cercanos\", tomando como m\u00e9trica de distancia la m\u00e9trica <code>XOR</code> propuesta por Kademlia.</p> <p>Al igual que Kademlia, se tratan los nodos como hojas de un \u00e1rbol binario, en el que la posici\u00f3n de cada nodo viene determinada por el prefijo \"\u00fanico m\u00e1s corto\" de su ID. Dichos nodos almacenan informaci\u00f3n de contacto entre s\u00ed para enrutar los mensajes de consulta.</p> <p>Se usa como base el protocolo de Kademlia que garantiza que cada nodo conoce al menos un nodo de cada uno de sus sub\u00e1rboles (si contiene alg\u00fan nodo); con esta garant\u00eda, cualquier nodo puede localizar a otro nodo por su ID.</p> <p>Se cuenta con un m\u00f3dulo de persistencia llamado <code>PersistentStorage</code> que funciona de la siguiente manera: Utiliza las rutas:</p> <ul> <li><code>static/metadata</code>: los nombres de los ficheros en esta ruta representan el hash de unos datos que se dividieron en varios chunks de tama\u00f1o m\u00e1ximo 1000kb. Contienen como valor listas de Python guardadas con pickle que tienen los hashes de cada chunk obtenido al picar unos datos determinados.</li> <li><code>static/keys</code>: los nombres de los ficheros en esta ruta representan todos los hashes de los datos almacenados, ya sea correspondiente a un dato completo o a un chunk de este. Contienen como valor los hashes correspondientes en bytes.</li> <li><code>static/values</code>: los nombres de los ficheros en esta ruta representan los hashes de todos los chunks que se han almacenado, excluyendo hashes de datos sin picar.</li> <li> <p><code>timestamps</code>: los nombres de los ficheros en esta ruta representan todos los hashes de los datos almacenados, al igual que la ruta de keys, pero contienen como valor un diccionario de Python guardado con pickle que tiene como llaves <code>date</code> con el valor <code>datetime.now()</code>, <code>republish</code> con un valor booleano y <code>last_write</code> que es un datetime que represante la ultima vez q se se sobreescribio el archivo. Esto se usa para llevar un registro de la \u00faltima vez que se accedi\u00f3 a `una llave, para saber si es necesario que el nodo que la contiene la republique porque es accedida con frecuencia y en caso de particion de la red mantener el valor escrito mas reciente y no la ultima informacion accedida, de este modo garantizando consistencia eventual.</p> </li> <li> <p>Cuando recibe un par <code>(llave, valor)</code>, ambos de tipo bytes, codifica la llave usando <code>base64.urlsafeb64encode</code> para poder obtener un string que se pueda usar como nombre de un fichero, se escribe un fichero con ese nombre en en la ruta de keys y de values, de manera tal que en el de key se escriba la llave en bytes y en el de values el valor en bytes. En el caso de que se especifique que el par a guardar es metadata el fichero que contendr\u00b4a al valor en bytes se escribe en la ruta de metadata. En ambos casos se crea un fichero en la ruta de timestamps con el nombre correspondiente.</p> </li> </ul> <p>El protocolo de Kademlia contiene cuatro RPCs : <code>PING</code> , <code>STORE</code> , <code>FINDNODE</code> y <code>FIND-VALUE</code>. Esta propuesta adem\u00b4as de usar estos cuatro RPCs, implementa <code>CONTAINS</code>, <code>BOOTSTRAPPABLE-NEIGHBOR</code>, <code>GET</code>, <code>GET-FILE-CHUNKS</code>, <code>UPLOAD-FILE</code>, <code>SET-KEY</code> <code>CHECK-IF-NEW-VALUE-EXIST</code>, <code>GET-FILE-CHUNK-VALUE</code>, <code>GET-FILE-CHUNK-LOCATION</code>, <code>FIND-CHUNK-LOCATION</code> y <code>FIND-NEIGHBORS</code>.</p> <p>El cliente solo utiliza los siguientes rpcs:</p> <ul> <li><code>GET</code>: Obtiene la informaci\u00b4on identificada con la llave de un archivo, devuelve una lista con las llaves de cada chunk</li> <li><code>UPLOAD-FILE</code>: Sube el archivo al sitema de ficheros, lo pica en chunks y guarda la metadata del archivo con la forma de unificar todos los chunks.</li> <li><code>GET-FIND-CHUNK-VALUE</code>: Devuelve el valor asociado al chunk</li> <li><code>GET-FIND-CHUNK-LOCATION</code>: Recibe la llave de un chunk y devuelve una tupla <code>(ip, puerto)</code> donde se encuentra.</li> <li><code>FIND-NEIGHBORS</code>: Devuelve los vecinos al nodo acorde a la cercania segun la metrica <code>XOR</code></li> </ul> <p>Los servidores ademas de los RPCs anteriores utilizan:</p> <ul> <li><code>CONTAINS</code>: Detecta si una llave est\u00b4a en un nodo, esto se utiliza tanto para la replicaci\u00b4on de la informaci\u00b4on como para encontrar si un nodo tiene la informaci\u00b4on que se desea</li> <li><code>GET-FILE-CHUNKS</code>: Obtiene la lista de ubicaciones de los chunks de la informaci\u00b4on.</li> <li><code>SET-KEY</code>: Guarda un par <code>(llave, valor)</code> en el sistema</li> </ul> <p>Se utiliz\u00b4o la misma estructura de tablas de enrutamiento que Kademlia, la tabla de enrutamiento es un \u00b4arbol binario cuyas hojas son k-buckets. Cada kbucket contiene nodos con alg\u00b4un prefijo com\u00b4un en sus ID. El prefijo es la posici\u00b4on del k-bucket en el \u00b4arbol binario, por lo que cada k-bucket cubre una parte del espacio de ID y, juntos, los k-buckets cubren todo el espacio de ID de 160 bits sin solaparse. Los nodos del \u00b4arbol de enrutamiento se asignan din\u00b4amicamente, seg\u00b4un sea necesario. Para garantizar la correcta replicaci\u00b4on de los datos, los nodos deben volver a publicar peri\u00b4odicamente las llaves. De lo contrario, dos fen\u00b4omenos pueden hacer que fallen las b\u00b4usquedas de llaves v\u00b4alidas. En primer lugar, algunos de los k nodos que obtienen inicialmente un par llave-valor cuando se publica pueden abandonar la red. En segundo lugar, pueden unirse a la red nuevos nodos con ID m\u00b4as cercanos a alguna llave publicada que los nodos en los que se public\u00b4o originalmente el par llave-valor. En ambos casos, los nodos con el par llave-valor deben volver a publicar para asegurarse de que est\u00b4a disponible en los k nodos m\u00b4as cercanos a la llave. Una vez que un cliente pide al sistema un determinado valor, se le devuelve una lista de las ubicaciones de los distintos chunks de datos (no necesariamente estos se encuentran en la misma PC) y se crea una conexi\u00b4on con la PC m\u00b4as cercana a la informaci\u00b4on de forma tal que la red no se sature innecesariamente. Luego el cliente unifica los datos y devuelve el valor almacenado. Una vez que un nodo env\u00b4\u0131a cierta informaci\u00b4on, marca ese archivo como pendiente de republicar y actualiza su timestamp, de manera que se informa a cada uno de los vecinos en los que se debe replicar la informaci\u00b4on que esta fue accedida y no ha de ser eliminada</p>"},{"location":"es/ejemplos/","title":"Ejemplos","text":""},{"location":"es/ejemplos/#guardar-un-valor","title":"Guardar un valor","text":""},{"location":"es/ejemplos/#nota-asegurarse-de-que-existe-al-menos-un-servidor-en-la-red","title":"Nota: Asegurarse de que existe al menos un servidor en la red","text":"<pre><code>from kade_drive.cli import ClientSession\nclient = ClientSession()\nclient.connect()\nclient.put(4, 5)\nvalue = client.get(4)\nassert value == 5\n</code></pre>"},{"location":"es/ejemplos/#guardar-un-dataframe","title":"Guardar un Dataframe","text":"<pre><code>    from sklearn.datasets import load_diabetes\nimport pandas as pd\nimport pickle\nfrom client import ClientSession\nfrom time import sleep\nX, y = load_diabetes(return_X_y=True)\nX = pd.DataFrame(X)\ny = pd.DataFrame(y)\nX_pickle = pickle.dumps(X)\ninitial_bootstrap_nodes = []\nclient_session = ClientSession(initial_bootstrap_nodes)\nclient_session.connect(use_broadcast_if_needed=True)\nclient_session.put(\"dataset_diabetes\", X_pickle)\nsleep(5)\nvalue_getted = client_session.get(\"dataset_diabetes\")\nassert X.equals(value_getted)\n</code></pre>"},{"location":"es/instalaci%C3%B3n/","title":"Instalaci\u00f3n","text":"<pre><code>pip install kade-drive\n</code></pre>"},{"location":"es/instalaci%C3%B3n/#server","title":"Server","text":"<pre><code>from kade_drive.server import start_server\nstart_server()\n</code></pre>"},{"location":"es/persistencia/","title":"Persistencia","text":"<p>La informaci\u00b4on por defecto est\u00b4a configurada para perdurar en el sistema por 1 semana si no se accede a ella, con fines demostrativos se utiliza 2 min para evaluar el correcto funcionamiento del sistema, en producci\u00b4on se deber\u00b4a analizar la ventana tiempo con la que eliminar los datos, es importante se\u02dcnalar que este sistema de ficheros est\u00b4a dise\u02dcnado para la interacci\u00b4on de los distintos nodos de la red y no como un sistema de persistencia de informaci\u00b4on, por lo que se decide eliminar la informaci\u00b4on que no se est\u00b4e utilizando para garantizar que nueva informaci\u00b4on puede ser almacenada por los algoritmos que se est\u00b4en entrenando. Este mecanismo se hace efectivo utilizando un thread cuyo \u00b4unico fin es verificar los timestamps de los ficheros y eliminar los que queden fuera de la ventana de tiempo predefinida.</p>"},{"location":"es/por-que-kademlia/","title":"\u00bfPor qu\u00e9 Kademlia?","text":"<p>Una de las ventajas de Kademlia es la eficiencia de su algoritmo de enrutamiento, lo que permite una comunicaci\u00b4on eficiente entre los nodos de la red. El prefijo binario de los ID de kademlia hace que se env\u00b4\u0131e la informaci\u00b4on hacia el nodo m\u00b4as cercano, minimizando la cantidad de saltos entre nodos y la latencia de la red en general. Otra de las ventajas de este protocolo es la capacidad de manejar fallos de nodos y particionado de la red. El mecanismo de republicaci\u00b4on de Kademlia garantiza que se mantengan actualizadas las tablas de ruta de los nodos, manteniendo as\u00b4\u0131 la conectividad en la red y evitando la p\u00b4erdida de datos dentro de lo posible. Los datos pueden estar dispersos en m\u00b4ultiples nodos de la red. Kademlia utiliza una tabla hash distribuida para mantener un registro de la ubicaci\u00b4on de los datos en la red. Esto permite que los nodos encuentren de manera eficiente la ubicaci\u00b4on de los datos que necesitan procesar o analizar. Adem\u00b4as, la arquitectura de Kademlia garantiza la tolerancia a fallos, lo que significa que si un nodo se desconecta o falla, los datos todav\u00b4\u0131a estar\u00b4an disponibles en otros nodos de la red. Esta capacidad de almacenamiento y recuperaci\u00b4on distribuida de datos de Kademlia es especialmente \u00b4util para los sistemas que suelen manejar grandes vol\u00b4umenes de datos que necesitan ser procesados en paralelo. Al distribuir los datos entre varios nodos, se puede lograr un procesamiento y an\u00b4alisis m\u00b4as r\u00b4apido y eficiente. En comparacion con alternativas como Chord, kademlia es una mejor opcion para aplicaciones que requieran un enrutamiento eficiente y frecuentes actualizaciones de informacion el cual se cree que sera el principal caso de uso de este sistema posterior a su integracion con Autogoal</p>"},{"location":"es/por-que-kademlia/#ejemplos-reales-del-uso-de-kademlia","title":"Ejemplos reales del uso de kademlia","text":"<p>Entre las compa\u02dcn\u00b4\u0131as que utilizan este protocolo se encuentran: - Storj: Plataforma de almacenamiento en la nube que en su versi\u00b4on 3 utiliz\u00b4o una versi\u00b4on modificada del protocolo para la implementaci\u00b4on de un sistema con capacidades similares a un DNS - Protocolo Ethereum: El protocolo Ethereum utiliza una versi\u00b4on ligeramente modificada de kademlia, manteniendo el m\u00b4etodo de identificaci\u00b4on con XOR y los K-buckets - The InterplanetaryFileSystem: En la implementaci\u00b4on de IPFS, el NodeID contiene un mapa directo hacia los hash de archivos de IPFS. Cada nodo tambi\u00b4en almacena informaci\u00b4on sobre d\u00b4onde obtener el archivo o recurso.</p>"},{"location":"es/recomendaciones/","title":"Recomendaciones","text":"<ul> <li>Dada la ca\u00b4\u0131da de un nodo de la red, mantener de alguna manera el id del nodo anterior para si su informaci\u00b4on se mantuviese a\u00b4un en disco y actualizada cargar con este id puede ser m\u00b4as eficiente que iniciar como un nodo nuevo y comanzar nuevamente el proceso de balanceo de la red.</li> <li>En la implementaci\u00b4on original de kademlia, la comunicaci\u00b4on entre nodos se realiza mediante <code>UDP</code>. Dada la limitaci\u00b4on de <code>UDP</code> para manejar grandes cantidades de informaci\u00b4on se cambi\u00b4o a <code>TCP</code>, pero esta es menos eficiente para las tareas que no sean transferencia de datos de almacenamiento. Implementar un doble protocolo de comunicaci\u00b4on entre los nodos deber\u00b4\u0131a representar una mejor\u00b4\u0131a en el rendimiento de la red.</li> <li>Cambiar el algoritmo de hash utilizado de sha1 a sha256 dado q sha1 ya no es considerado seguro y sus vulnerabilidades hacen mas facil a los atacantes realizar actividades maliciosas</li> <li>Implementar un mecanismo de testing automatico distribuido utilizando tecnicas como <code>chaos testing</code> y <code>swarm testing</code></li> </ul>"},{"location":"es/replicacion/","title":"Replicaci\u00f3n","text":"<p>Para algoritmo de republicaci\u00b4on implementado se utiliza un Thread que se ejecuta cada un intervalo de tiempo i y consiste en lo siguiente:</p> <ul> <li>Recorrer todas las llaves en el storage cuyo timestamp sea mayor que un tiempo t y aquellas que tengan republish en True son republicadas por el nodo.</li> <li>Recorrer todas las llaves en el storage y verificar en la red cu\u00b4antas r\u00b4eplicas puede encontrar de cada una, aquellas llaves que est\u00b4en por debajo del factor de replicaci\u00b4on especificado son republicadas por el nodo.</li> </ul>"},{"location":"es/tolerancia-fallas/","title":"Tolerancia a Fallas","text":"<p>Los nodos mantienen informaci\u00b4on sobre otros nodos cercanos en la red, almacenando estos nodos en su tabla de enrutamiento. Cada nodo tiene una lista de <code>k-buckets</code> que contienen los detalles de contacto de otros nodos en la red, clasificados seg\u00b4un su proximidad en el espacio de identificaci\u00b4on. Cuando un nodo deja de responder o se desconecta, los otros nodos de la red detectan la falta de respuesta despu\u00b4es de un per\u00b4\u0131odo de tiempo determinado. En ese momento, se considera que el nodo fallido est\u00b4a inactivo. Cuando un nodo detecta la inactividad de otro nodo, actualiza su tabla de enrutamiento eliminando la entrada del nodo fallido. Adem\u00b4as, el nodo realiza una serie de acciones para mantener la conectividad y la redundancia en la red.</p> <ul> <li> <p>Replicaci\u00b4on de datos: Si el nodo fallido almacenaba datos, otros nodos de la red pueden tomar la responsabilidad de mantener r\u00b4eplicas de esos datos. De esta manera, los datos permanecen disponibles incluso si el nodo original se desconecta. Los nodos que asumir\u00b4an esta responsabilidad ser\u00b4an los que est\u00b4en m\u00b4as cercanos utilizando la misma m\u00b4etrica de XOR con los IDs para mantener la red lo m\u00b4as optimizada posible.</p> </li> <li> <p>Actualizaci\u00b4on de informaci\u00b4on de enrutamiento: Los nodos que ten\u00b4\u0131an al nodo fallido en su tabla de enrutamiento actualizan esa entrada elimin\u00b4andola. De esta manera, los nodos evitan enviar mensajes o realizar acciones hacia un nodo que ya no est\u00e1 disponible.</p> </li> <li> <p>La expiraci\u00b4on de contacto garantiza que la informaci\u00b4on almacenada en la red permanezca accesible incluso cuando los nodos individuales fallan. Al eliminar los nodos inactivos de los buckets, se asegura de que las rutas de enrutamiento se actualicen y se mantengan eficientes</p> </li> </ul>"}]}