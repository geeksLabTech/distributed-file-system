{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"analisis-cap/","title":"An\u00e1lisis del teorema CAP","text":"<p>El teorema CAP, tambi\u00b4en conocido como el teorema de Brewer, es un concepto fundamental en los sistemas distribuidos que establece que es imposible que un almac\u00b4en de datos distribuido proporcione simult\u00b4aneamente consistencia (C), disponibilidad (A) y tolerancia a particiones (P).</p> <ul> <li> <p>Consistencia (C): La consistencia se refiere a que todos los nodos en un sistema distribuido tengan la misma visi\u00b4on de los datos al mismo tiempo. En este sistema, lograr una consistencia estricta en todos los nodos no es una prioridad. En cambio, se garantiza en la consistencia eventual, lo que significa que con el tiempo, todos los nodos converger\u00b4an al mismo estado. Los nodos intercambian peri\u00b4odicamente informaci\u00b4on y actualizan sus tablas de enrutamiento para lograr esta convergencia.</p> </li> <li> <p>Disponibilidad (A): La disponibilidad implica que el sistema permanezca receptivo y accesible para los usuarios incluso en presencia de fallos de nodos o particiones de red. EL sistema prioriza la disponibilidad asegurando que los nodos puedan seguir operando y proporcionando servicios incluso cuando algunos nodos est\u00b4en no disponibles o inalcanzables. Esto se logra a trav\u00b4es de la redundancia y la replicaci\u00b4on de datos, donde se almacenan m\u00b4ultiples copias de datos en diferentes nodos.</p> </li> <li> <p>Tolerancia a particiones (P): La tolerancia a particiones se refiere a la capacidad del sistema para continuar funcionando y proporcionar servicios a pesar de particiones o fallos en la red. El sistema est\u00b4a dise\u02dcnado para ser tolerante a particiones, lo que permite que la red siga operando y mantenga su funcionalidad incluso cuando los nodos est\u00b4en temporalmente desconectados o aislados debido a problemas de red.</p> </li> </ul> <p>Todos estos puntos se analizan asumiendo que no se sobrepasa la capacidad de recuperaci\u00b4on del sistema, si se superase dicho punto el sistema comenzar\u00b4a a ver afectada la disponibilidad. Pero esto solo Ocurrir\u00b4\u0131a en caso de fallos simult\u00b4aneos, puesto que el sistema es capaz de recuperarse de fallos eventuales sin ning\u00b4un problema.</p>"},{"location":"autodescubrimiento/","title":"Autodescubrimiento","text":"<p>Para el mecanismo de auto descubrimiento se crea tambi\u00b4en un Thread que emite un latido en las direcciones de broadcast de cada una de las NICs del host, con un identificador de la fuente del broadcast e ip y puerto \u201ddfs ip puerto\u201d. Cada vez que un nuevo nodo entra a la red escucha los broadcast para encontrar vecinos. El mecanismo es accesible desde el cliente, haciendo transparente la conexi\u00b4on para el usuario.</p>"},{"location":"autodescubrimiento/#desventajas-de-este-enfoque","title":"Desventajas de este enfoque","text":"<p>Como se utiliza broadcast para el autodescubrimiento esto solo es posible en una red local, o utilizando una (o varias) PC como puente entre distintas subredes, haciendo que si por alguna raz\u00b4on no existiese una forma de conectar localmente las computadoras, estas no ser\u00b4an capaces de encontrarse, no obstante como el sistema est\u00b4a pensado para conexi\u00b4on de distintas estaciones de trabajo se asume que estas estar\u00b4an en la misma red local haciendo factible este enfoque.</p>"},{"location":"cliente/","title":"Cliente","text":"<p>Se desarroll\u00f3 un cliente que cuenta con las siguientes caracter\u00edsticas: - Puede recibir puntos de entrada a la red, conocidos como bootstrap nodes para una conexi\u00b4on directa con el sistema de ficheros - Posee un mecanismo de auto-descubrimiento recibiendo broadcast por todas las NICs de la pc, si no recibe ning\u00b4un bootstrap node o no es capaz de conectar con ninguno puede usar esta funcionalidad para encontrar alg\u00b4un nodo autom\u00b4aticamente. - Tiene la capacidad de al conectar con alg\u00b4un nodo descubir otros nodos a partir de los vecinos de este de manera autom\u00b4atica. - Maneja errores relacionados con inestabilidad de la red o ca\u00b4\u0131das inesperadas de un nodo, permitiendo que el usuario establezca el n\u00b4umero de reintentos que se debe hacer cuando se pierde la conexi\u00b4on con un nodo, posee un cola con los nodos conocidos que al agotarse el n\u00b4umero de reintentos de conexi\u00b4on con un nodo, este es removido de la cola y se pasa al siguiente, es posible especificar tambi\u00b4en si se quiere que se utilice el mecanismo de auto-descubribimiento al quedarse la cola vac\u00b4\u0131a.</p>"},{"location":"descripcion/","title":"Descripci\u00f3n del sistema","text":"<p>Al igual que muchos sistemas de tabla de hash distribuida, las llaves se almacenan en 160 bits. Cada uno de los ordenadores participantes en la red tiene un ID, y los pares (llave, valor) son almacenados en nodos con ID \"cercanos\", tomando como m\u00e9trica de distancia la m\u00e9trica XOR propuesta por Kademlia.</p> <p>Al igual que Kademlia, se tratan los nodos como hojas de un \u00e1rbol binario, en el que la posici\u00f3n de cada nodo viene determinada por el prefijo \"\u00fanico m\u00e1s corto\" de su ID. Dichos nodos almacenan informaci\u00f3n de contacto entre s\u00ed para enrutar los mensajes de consulta.</p> <p>Se usa como base el protocolo de Kademlia que garantiza que cada nodo conoce al menos un nodo de cada uno de sus sub\u00e1rboles (si contiene alg\u00fan nodo); con esta garant\u00eda, cualquier nodo puede localizar a otro nodo por su ID.</p> <p>Se cuenta con un m\u00f3dulo de persistencia llamado PersistentStorage que funciona de la siguiente manera: Utiliza las rutas:</p> <ul> <li>static/metadata: los nombres de los ficheros en esta ruta representan el hash de unos datos que se dividieron en varios chunks de tama\u00f1o m\u00e1ximo 1000kb. Contienen como valor listas de Python guardadas con pickle que tienen los hashes de cada chunk obtenido al picar unos datos determinados.</li> <li>static/keys: los nombres de los ficheros en esta ruta representan todos los hashes de los datos almacenados, ya sea correspondiente a un dato completo o a un chunk de este. Contienen como valor los hashes correspondientes en bytes.</li> <li>static/values: los nombres de los ficheros en esta ruta representan los hashes de todos los chunks que se han almacenado, excluyendo hashes de datos sin picar.</li> <li> <p>timestamps: los nombres de los ficheros en esta ruta representan todos los hashes de los datos almacenados, al igual que la ruta de keys, pero contienen como valor un diccionario de Python guardado con pickle que tiene como llaves \"date\" con el valor \"datetime.now()\" y \"republish\" con un valor booleano. Esto se usa para llevar un registro de la \u00faltima vez que se accedi\u00f3 a `una llave y para saber si es necesario que el nodo que la contiene la republique porque es accedida con frecuencia.</p> </li> <li> <p>Cuando recibe un par (llave, valor), ambos de tipo bytes, codifica la llave usando base64.urlsafeb64encode para poder obtener un string que se pueda usar como nombre de un fichero, se escribe un fichero con ese nombre en en la ruta de keys y de values, de manera tal que en el de key se escriba la llave en bytes y en el de values el valor en bytes. En el caso de que se especifique que el par a guardar es metadata el fichero que contendr\u00b4a al valor en bytes se escribe en la ruta de metadata. En ambos casos se crea un fichero en la ruta de timestamps con el nombre correspondiente.</p> </li> </ul> <p>El protocolo de Kademlia contiene cuatro RPCs : PING , STORE , FINDNODE y FIND-VALUE[2]. Esta propuesta adem\u00b4as de usar estos cuatro RPCs, implementa CONTAINS, BOOTSTRAPPABL-NEIGHBOR, GET, GET-FILECHUNKS, UPLOAD-FILE, SET-KEY y FIND-NEIGHBORS</p> <ul> <li>CONTAINS: Detecta si una llave est\u00b4a en un nodo, esto se utiliza tanto para la replicaci\u00b4on de la informaci\u00b4on como para encontrar si un nodo tiene la informaci\u00b4on que se desea</li> <li>BOOTSTRAPPABLE-NEIGHBOR</li> <li>GET: Obtiene la informaci\u00b4on identificada con la llave de un chunk</li> <li>GET-FILE-CHUNKS: Obtiene la lista de ubicaciones de los chunks de la informaci\u00b4on.</li> <li>UPLOAD-FILE: Sube el archivo al sitema de ficheros, lo pica en los chunks y guarda la metadata del archivo con la forma de unificar todos los ficheros.</li> <li>SET-KEY</li> <li>FIND-NEIGHBORS:</li> </ul> <p>Se utiliz\u00b4o la misma estructura de tablas de enrutamiento que Kademlia, la tabla de enrutamiento es un \u00b4arbol binario cuyas hojas son k-buckets. Cada kbucket contiene nodos con alg\u00b4un prefijo com\u00b4un en sus ID. El prefijo es la posici\u00b4on del k-bucket en el \u00b4arbol binario, por lo que cada k-bucket cubre una parte del espacio de ID y, juntos, los k-buckets cubren todo el espacio de ID de 160 bits sin solaparse. Los nodos del \u00b4arbol de enrutamiento se asignan din\u00b4amicamente, seg\u00b4un sea necesario. Para garantizar la correcta replicaci\u00b4on de los datos, los nodos deben volver a publicar peri\u00b4odicamente las llaves. De lo contrario, dos fen\u00b4omenos pueden hacer que fallen las b\u00b4usquedas de llaves v\u00b4alidas. En primer lugar, algunos de los k nodos que obtienen inicialmente un par llave-valor cuando se publica pueden abandonar la red. En segundo lugar, pueden unirse a la red nuevos nodos con ID m\u00b4as cercanos a alguna llave publicada que los nodos en los que se public\u00b4o originalmente el par llave-valor. En ambos casos, los nodos con el par llave-valor deben volver a publicar para asegurarse de que est\u00b4a disponible en los k nodos m\u00b4as cercanos a la llave. Una vez que un cliente pide al sistema un determinado valor, se le devuelve una lista de las ubicaciones de los distintos chunks de datos (no necesariamente estos se encuentran en la misma PC) y se crea una conexi\u00b4on con la PC m\u00b4as cercana a la informaci\u00b4on de forma tal que la red no se sature innecesariamente. Luego el cliente unifica los datos y devuelve el valor almacenado. Una vez que un nodo env\u00b4\u0131a cierta informaci\u00b4on, marca ese archivo como pendiente de republicar y actualiza su timestamp, de manera que se informa a cada uno de los vecinos en los que se debe replicar la informaci\u00b4on que esta fue accedida y no ha de ser eliminada</p>"},{"location":"introducci%C3%B3n/","title":"Introducci\u00f3n","text":"<p>Este proyecto utiliza como punto de partida Kademlia. Kademlia es una tabla de hash distribuida, que permite que millones de ordenadores se organicen autom\u00e1ticamente en una red, se comuniquen con otros ordenadores de la red y compartan recursos.</p> <p>Se hace uso de varios enfoques e ideas propuestas por Kademlia, muchas de ellas se usan como base para ampliar las funcionalidades del proyecto y otras son utilizadas de la misma manera que Kademlia.</p> <p>Una vez que se tiene un sistema de ficheros se integra a AutoGOAL para resolver problemas de aprendizaje de m\u00e1quina de una manera m\u00e1s eficiente en cuanto a rendimiento, uso de la red y almacenamiento.</p>"},{"location":"persistencia/","title":"Persistencia","text":"<p>La informaci\u00b4on por defecto est\u00b4a configurada para perdurar en el sistema por 1 semana si no se accede a ella, con fines demostrativos se utiliza 2 min para evaluar el correcto funcionamiento del sistema, en producci\u00b4on se deber\u00b4a analizar la ventana tiempo con la que eliminar los datos, es importante se\u02dcnalar que este sistema de ficheros est\u00b4a dise\u02dcnado para la interacci\u00b4on de los distintos nodos de la red y no como un sistema de persistencia de informaci\u00b4on, por lo que se decide eliminar la informaci\u00b4on que no se est\u00b4e utilizando para garantizar que nueva informaci\u00b4on puede ser almacenada por los algoritmos que se est\u00b4en entrenando. Este mecanismo se hace efectivo utilizando un thread cuyo \u00b4unico fin es verificar los timestamps de los ficheros y eliminar los que queden fuera de la ventana de tiempo predefinida.</p>"},{"location":"por-que-kademlia/","title":"\u00bfPor qu\u00e9 Kademlia?","text":"<p>Una de las ventajas de Kademlia es la eficiencia de su algoritmo de enrutamiento, lo que permite una comunicaci\u00b4on eficiente entre los nodos de la red. El prefijo binario de los ID de kademlia hace que se env\u00b4\u0131e la informaci\u00b4on hacia el nodo m\u00b4as cercano, minimizando la cantidad de saltos entre nodos y la latencia de la red en general. Otra de las ventajas de este protocolo es la capacidad de manejar fallos de nodos y particionado de la red. El mecanismo de refrescado de Kademlia garantiza que se mantengan actualizadas las tablas de ruta de los nodos, manteniendo as\u00b4\u0131 la conectividad en la red y evitando la p\u00b4erdida de datos dentro de lo posible. Los datos pueden estar dispersos en m\u00b4ultiples nodos de la red. Kademlia utiliza una tabla hash distribuida para mantener un registro de la ubicaci\u00b4on de los datos en la red. Esto permite que los nodos encuentren de manera eficiente la ubicaci\u00b4on de los datos que necesitan procesar o analizar. Adem\u00b4as, la arquitectura de Kademlia garantiza la tolerancia a fallos, lo que significa que si un nodo se desconecta o falla, los datos todav\u00b4\u0131a estar\u00b4an disponibles en otros nodos de la red. Esta capacidad de almacenamiento y recuperaci\u00b4on distribuida de datos de Kademlia es especialmente \u00b4util para los sistemas que suelen manejar grandes vol\u00b4umenes de datos que necesitan ser procesados en paralelo. Al distribuir los datos entre varios nodos, se puede lograr un procesamiento y an\u00b4alisis m\u00b4as r\u00b4apido y eficiente.</p>"},{"location":"por-que-kademlia/#ejemplos-reales-del-uso-de-kademlia","title":"Ejemplos reales del uso de kademlia","text":"<p>Entre las compa\u02dcn\u00b4\u0131as que utilizan este protocolo se encuentran: - Storj: Plataforma de almacenamiento en la nube que en su versi\u00b4on 3 utiliz\u00b4o una versi\u00b4on modificada del protocolo para la implementaci\u00b4on de un sistema con capacidades similares a un DNS - Protocolo Ethereum: El protocolo Ethereum utiliza una versi\u00b4on ligeramente modificada de kademlia, manteniendo el m\u00b4etodo de identificaci\u00b4on con XOR y los K-buckets - The InterplanetaryFileSystem: En la implementaci\u00b4on de IPFS, el NodeID contiene un mapa directo hacia los hash de archivos de IPFS. Cada nodo tambi\u00b4en almacena informaci\u00b4on sobre d\u00b4onde obtener el archivo o recurso.</p>"},{"location":"recomendaciones/","title":"Recomendaciones","text":"<ul> <li>Dada la ca\u00b4\u0131da de un nodo de la red, mantener de alguna manera el id del nodo anterior para si su informaci\u00b4on se mantuviese a\u00b4un en disco y actualizada cargar con este id puede ser m\u00b4as eficiente que iniciar como un nodo nuevo y comanzar nuevamente el proceso de balanceo de la red.</li> <li>En la implementaci\u00b4on original de kademlia, la comunicaci\u00b4on entre nodos se realiza mediante UDP. Dada la limitaci\u00b4on de UDP para manejar grandes cantidades de informaci\u00b4on se cambi\u00b4o a TCP, pero esta es menos eficiente para las tareas que no sean transferencia de datos de almacenamiento. Implementar un doble protocolo de comunicaci\u00b4on entre los nodos deber\u00b4\u0131a representar una mejor\u00b4\u0131a en el rendimiento de la red.</li> </ul>"},{"location":"replicacion/","title":"Replicaci\u00f3n","text":"<p>Para algoritmo de republicaci\u00b4on implementado se utiliza un Thread que se ejecuta cada un intervalo de tiempo i y consiste en lo siguiente: - Recorrer todas las llaves en el storage cuyo timestamp sea mayor que un tiempo t y aquellas que tengan republish en True son republicadas por el nodo. - Recorrer todas las llaves en el storage y verificar en la red cu\u00b4antas r\u00b4eplicas puede encontrar de cada una, aquellas llaves que est\u00b4en por debajo del factor de replicaci\u00b4on especificado son republicadas por el nodo.</p>"},{"location":"tolerancia-fallas/","title":"Tolerancia a Fallas","text":"<p>Los nodos mantienen informaci\u00b4on sobre otros nodos cercanos en la red, almacenando estos nodos en su tabla de enrutamiento. Cada nodo tiene una lista de \u201dk-buckets\u201dque contienen los detalles de contacto de otros nodos en la red, clasificados seg\u00b4un su proximidad en el espacio de identificaci\u00b4on. Cuando un nodo deja de responder o se desconecta, los otros nodos de la red detectan la falta de respuesta despu\u00b4es de un per\u00b4\u0131odo de tiempo determinado. En ese momento, se considera que el nodo fallido est\u00b4a inactivo. Cuando un nodo detecta la inactividad de otro nodo, actualiza su tabla de enrutamiento eliminando la entrada del nodo fallido. Adem\u00b4as, el nodo realiza una serie de acciones para mantener la conectividad y la redundancia en la red.</p> <ul> <li> <p>Replicaci\u00b4on de datos: Si el nodo fallido almacenaba datos, otros nodos de la red pueden tomar la responsabilidad de mantener r\u00b4eplicas de esos datos. De esta manera, los datos permanecen disponibles incluso si el nodo original se desconecta. Los nodos que asumir\u00b4an esta responsabilidad ser\u00b4an los que est\u00b4en m\u00b4as cercanos utilizando la misma m\u00b4etrica de XOR con los IDs para mantener la red lo m\u00b4as optimizada posible.</p> </li> <li> <p>Transferencia de responsabilidad: Si el nodo fallido era responsable de ciertos identificadores en la red, otros nodos toman la responsabilidad de manejarlos. Esto asegura que no se pierda el acceso a los datos o recursos.</p> </li> <li> <p>Actualizaci\u00b4on de informaci\u00b4on de enrutamiento: Los nodos que ten\u00b4\u0131an al nodo fallido en su tabla de enrutamiento actualizan esa entrada elimin\u00b4andola. De esta manera, los nodos evitan enviar mensajes o realizar acciones hacia un nodo que ya no est\u00e1 disponible.</p> </li> </ul> <p>La expiraci\u00b4on de contacto garantiza que la informaci\u00b4on almacenada en la red permanezca accesible incluso cuando los nodos individuales fallan. Al eliminar los nodos inactivos de los buckets, se asegura de que las rutas de enrutamiento se actualicen y se mantengan eficientes</p>"}]}