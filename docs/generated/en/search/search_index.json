{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"autodiscovery/","title":"5 - Autodiscovery","text":"<p>For the autodiscovery mechanism, a <code>Thread</code> is also created that sends a heartbeat to the broadcast addresses of each NIC (Network Interface Card) on the host, with a broadcast source identifier and the IP and port \"<code>dfs ip port</code>\". Whenever a new node joins the network, it listens to the broadcasts to discover neighbors. This mechanism is accessible from the client, making the connection transparent to the user.</p>"},{"location":"autodiscovery/#disadvantages-of-this-approach","title":"Disadvantages of this approach","text":"<p>Since broadcast is used for autodiscovery, it is only possible within a local network or by using one (or multiple) PCs as a bridge between different subnets. If, for some reason, there is no way to connect computers locally, they will not be able to discover each other. However, since the system is designed for connecting different workstations, it is assumed that they will be on the same local network, making this approach feasible.</p>"},{"location":"cap-analysis/","title":"9 - CAP Theorem Analysis","text":"<p>The CAP theorem, also known as Brewer's theorem, is a fundamental concept in distributed systems that states that it is impossible for a distributed data store to simultaneously provide consistency (C), availability (A), and partition tolerance (P).</p> <ul> <li> <p>Consistency (C): Consistency refers to the requirement that all nodes in a distributed system have the same view of the data at the same time. In this system, achieving strict consistency across all nodes is not a priority. Instead, eventual consistency is guaranteed, which means that over time, all nodes will converge to the same state. Nodes periodically exchange information and update their routing tables to achieve this convergence.</p> </li> <li> <p>Availability (A): Availability implies that the system remains responsive and accessible to users even in the presence of node failures or network partitions. The system prioritizes availability by ensuring that nodes can continue to operate and provide services even when some nodes are unavailable or unreachable. This is achieved through redundancy and data replication, where multiple copies of data are stored on different nodes.</p> </li> <li> <p>Partition Tolerance (P): Partition tolerance refers to the system's ability to continue functioning and providing services despite network partitions or failures. The system is designed to be partition-tolerant, allowing the network to operate and maintain its functionality even when nodes are temporarily disconnected or isolated due to network issues.</p> </li> </ul> <p>All these points are analyzed assuming that the system's recovery capacity is not exceeded. If that threshold were to be surpassed, the system's availability would be affected. However, this would only occur in the case of simultaneous failures, as the system is capable of recovering from occasional failures without any issues.</p>"},{"location":"client/","title":"4 - Client","text":"<p>A client has been developed with the following features:</p> <ul> <li>It can receive entry points to the network, known as bootstrap nodes, for a direct connection to the file system.</li> <li>It has an autodiscovery mechanism that listens to broadcasts on all NICs (Network Interface Cards) of the PC. If it doesn't receive any bootstrap node or is unable to connect to any, it can use this functionality to automatically discover a node.</li> <li>It has the ability to discover other nodes automatically by connecting to a node and exploring its neighbors.</li> <li>It handles errors related to network instability or unexpected node failures. The user can specify the number of connection retries when a connection to a node is lost. It maintains a queue of known nodes, and when the maximum number of connection retries is reached for a node, it is removed from the queue and the next node is used. It is also possible to specify whether the autodiscovery mechanism should be used when the queue becomes empty.</li> </ul>"},{"location":"description/","title":"2 - Description","text":"<p>The system follows the approach of many distributed hash table systems, where keys are stored as 160-bit values. Each participating computer in the network has an ID, and the (key, value) pairs are stored on nodes with \"close\" IDs, using the XOR metric proposed by Kademlia as the distance metric.</p> <p>Similar to Kademlia, nodes are treated as leaves in a binary tree, where the position of each node is determined by the \"shortest unique prefix\" of its ID. These nodes store contact information for each other to route query messages.</p> <p>The system is based on the Kademlia protocol, which guarantees that each node knows at least one node from each of its subtrees (if they contain any nodes). With this guarantee, any node can locate another node by its ID.</p> <p>There is a persistence module called PersistentStorage that operates as follows: It uses the following paths:</p> <ul> <li>static/metadata: The file names in this path represent the hash of data that has been divided into multiple chunks with a maximum size of 1000kb. Each file contains a Python list stored with pickle, which holds the hashes of each chunk obtained from the data.</li> <li>static/keys: The file names in this path represent the hashes of stored data, whether it corresponds to a complete data or a chunk of it. Each file contains the corresponding hashes in bytes.</li> <li>static/values: The file names in this path represent the hashes of all the chunks that have been stored, excluding hashes of unchunked data.</li> <li>timestamps: The file names in this path represent the hashes of stored data, similar to the keys path. However, each file contains a Python dictionary stored with pickle, which has keys \"date\" with the value \"datetime.now()\" and \"republish\" with a boolean value. This is used to keep track of the last access time of a key and to determine if the node holding the key needs to republish it because it is frequently accessed.</li> </ul> <p>When receiving a (key, value) pair, both of type bytes, the key is encoded using base64.urlsafeb64encode to obtain a string that can be used as a file name. A file is then written with that name in the keys and values paths. In the keys file, the key is written as bytes, and in the values file, the value is written as bytes. If the pair to be stored is metadata, the file containing the value as bytes is written in the metadata path. In both cases, a file is created in the timestamps path with the corresponding name.</p> <p>The Kademlia protocol includes four RPCs: PING, STORE, FINDNODE, and FINDVALUE. In addition to using these four RPCs, this proposal implements the following:</p> <ul> <li>CONTAINS: Determines if a key exists in a node. This is used for both information replication and to find out if a node has the desired information.</li> <li>BOOTSTRAPPABLE-NEIGHBOR</li> <li>GET: Retrieves the information identified by the key of a chunk.</li> <li>GET-FILE-CHUNKS: Retrieves the list of locations of the chunks that make up the information.</li> <li>UPLOAD-FILE: Uploads a file to the file system, divides it into chunks, and stores the metadata of the file to unify all the files.</li> <li>SET-KEY</li> <li>FIND-NEIGHBORS</li> </ul> <p>The same routing table structure as Kademlia is used. The routing table is a binary tree whose leaves are k-buckets. Each k-bucket contains nodes with a common prefix in their IDs. The prefix represents the position of the k-bucket in the binary tree, so each k-bucket covers a portion of the 160-bit ID space, and together, the k-buckets cover the entire ID space without overlap. The nodes of the routing tree are dynamically assigned as needed.</p> <p>To ensure proper data replication, nodes need to periodically republish keys. Otherwise, two phenomena can cause valid key searches to fail. First, some of the k nodes that initially obtain a key-value pair when it is published may leave the network. Second, new nodes may join the network with IDs closer to a published key than the nodes on which the key-value pair was originally published. In both cases, nodes with the key-value pair need to republish to ensure it is available on the k nodes closest to the key.</p> <p>When a client requests a specific value from the system, it receives a list of locations of the different data chunks (which may not be on the same PC). A connection is established with the PC closest to the information to avoid unnecessary network congestion. The client then combines the data and returns the stored value. Once a node sends certain information, that file is marked as pending for republication, and its timestamp is updated. This ensures that each of the neighboring nodes that need to replicate the information is informed that it has been accessed and should not be deleted.</p>"},{"location":"fault-tolerance/","title":"Fault Tolerance","text":"<p>Nodes maintain information about other nodes in the network by storing them in their routing table. Each node has a list of \"k-buckets\" that contain the contact details of other nodes in the network, classified based on their proximity in the ID space.</p> <p>When a node becomes unresponsive or disconnects, other nodes in the network detect the lack of response after a certain period of time. At that point, the failed node is considered inactive.</p> <p>When a node detects the inactivity of another node, it updates its routing table by removing the entry of the failed node. Additionally, the node performs a series of actions to maintain connectivity and redundancy in the network.</p> <ul> <li> <p>Data Replication: If the failed node was storing data, other nodes in the network can take the responsibility of maintaining replicas of that data. This ensures that the data remains available even if the original node disconnects. The nodes that will assume this responsibility are those that are closest using the same XOR metric with IDs to keep the network as optimized as possible.</p> </li> <li> <p>Responsibility Transfer: If the failed node was responsible for certain identifiers in the network, other nodes take over the responsibility of handling them. This ensures that access to the data or resources is not lost.</p> </li> <li> <p>Routing Information Update: Nodes that had the failed node in their routing table update that entry by removing it. This way, nodes avoid sending messages or performing actions towards a node that is no longer available.</p> </li> </ul> <p>The expiration of contact ensures that the information stored in the network remains accessible even when individual nodes fail. By removing inactive nodes from the buckets, it ensures that routing paths are updated and kept efficient.</p>"},{"location":"introduction/","title":"Introduction","text":"<p>This project builds upon the foundation of Kademlia. Kademlia is a distributed hash table that enables millions of computers to self-organize into a network, communicate with each other, and share resources.</p> <p>Several approaches and ideas proposed by Kademlia are utilized in this project, serving as the basis to expand the functionalities of the system, while others are employed in a similar manner as Kademlia.</p> <p>Once the file system is established, it is integrated with AutoGOAL to tackle machine learning problems in a more efficient manner in terms of performance, network utilization, and storage.</p>"},{"location":"persistence/","title":"Persistence","text":"<p>By default, the information in the system is configured to persist for 1 week if it is not accessed. For demonstration purposes, a 2-minute window is used to evaluate the correct functioning of the system. In a production environment, the time window for data deletion should be carefully analyzed. It is important to note that this file system is designed for interaction among the different nodes in the network and not as a persistent storage system. Therefore, information that is not being used is periodically deleted to ensure that new information can be stored for training algorithms. This mechanism is implemented through a dedicated thread that checks the timestamps of the files and deletes those that fall outside the predefined time window.</p>"},{"location":"recommendations/","title":"Recommendations","text":"<ul> <li> <p>In the event of a node failure, it is recommended to keep track of the ID of the previous node. If its information is still available on disk and up to date, loading it with this ID can be more efficient than starting as a new node and going through the network rebalancing process again.</p> </li> <li> <p>In the original implementation of Kademlia, node communication was done using UDP. However, UDP has limitations in handling large amounts of data. It was changed to TCP, which is less efficient for tasks other than data storage transfer. Implementing a dual communication protocol between nodes could improve network performance.</p> </li> </ul>"},{"location":"replication/","title":"Replication","text":"<p>The implemented replication algorithm uses a Thread that runs at a specified time interval (i) and performs the following steps:</p> <ol> <li> <p>Iterate through all the keys in the storage whose timestamp is greater than a given time (t). Republish the keys that have the \"republish\" flag set to True.</p> </li> <li> <p>Iterate through all the keys in the storage and check how many replicas of each key can be found in the network. If the number of replicas is below the specified replication factor, the node republishes those keys.</p> </li> </ol>"},{"location":"why-kademlia/","title":"Why Kademlia?","text":"<p>One of the advantages of Kademlia is the efficiency of its routing algorithm, which allows for efficient communication between nodes in the network. The binary prefix of Kademlia IDs ensures that information is sent to the closest node, minimizing the number of hops and network latency overall.</p> <p>Another advantage of this protocol is its ability to handle node failures and network partitioning. Kademlia's refreshing mechanism ensures that node routing tables stay up to date, maintaining network connectivity and minimizing data loss as much as possible.</p> <p>Data can be distributed across multiple nodes in the network. Kademlia uses a distributed hash table to keep track of data locations in the network. This allows nodes to efficiently find the location of data they need to process or analyze. Additionally, Kademlia's architecture ensures fault tolerance, meaning that if a node disconnects or fails, the data will still be available on other nodes in the network. This distributed storage and retrieval capability of Kademlia is particularly useful for systems that handle large volumes of data requiring parallel processing. By distributing the data among multiple nodes, faster and more efficient processing and analysis can be achieved.</p>"},{"location":"why-kademlia/#real-world-examples-of-kademlia-usage","title":"Real-World Examples of Kademlia Usage","text":"<p>Some companies that utilize this protocol include:</p> <ul> <li>Storj: A cloud storage platform that used a modified version of the Kademlia protocol in its version 3 for implementing a system with DNS-like capabilities.</li> <li>Ethereum Protocol: The Ethereum protocol uses a slightly modified version of Kademlia, maintaining the XOR-based ID identification and K-buckets.</li> <li>The Interplanetary FileSystem (IPFS): In the implementation of IPFS, the NodeID contains a direct map to IPFS file hashes. Each node also stores information on where to obtain the file or resource.</li> </ul>"}]}